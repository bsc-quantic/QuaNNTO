{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f3481e3e",
      "metadata": {},
      "source": [
        "# QuaNNTO Example — Learning a Cubic-Phase Gate Dataset with Hybrid CV-QONNs\n",
        "\n",
        "This notebook is an **example script** for the `QuaNNTO` library (branch `revisited-code`).\n",
        "\n",
        "## Goal\n",
        "Learn a dataset representing the action/statistics of a **non-Gaussian gate** (here: a *cubic phase* operation) using a **hybrid-trained CV-QONN**, while sweeping the number of optical modes:\n",
        "- `N = 2, 3, 4`\n",
        "\n",
        "The dataset is assumed to have been **pre-generated** and stored under the configured datasets directory.\n",
        "\n",
        "---\n",
        "## What this notebook does\n",
        "1. Loads a precomputed training dataset: `fock_cubicphase_gamma{gamma}_...`.\n",
        "2. Trains one CV-QONN per `N` using `hybrid_build_and_train_model`.\n",
        "3. Evaluates each trained model on the training set (since we do not use a separate validation/test set here).\n",
        "4. Saves losses and predicted outputs using `quannto.utils.path_utils`.\n",
        "5. Plots training loss curves for all models.\n",
        "\n",
        "---\n",
        "## Notes\n",
        "- `observable='cubicphase'` is treated as a **special readout mode** in QuaNNTO for this dataset/task.\n",
        "- The original script contained mis-decoded characters for operator labels (`â†`, `â`).\n",
        "  Here we use LaTeX-friendly labels: $\\hat a^\\dagger$ (addition) and $\\hat a$ (subtraction).\n",
        "- This notebook uses QuaNNTO path utilities so results do not depend on the notebook working directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ca02b3f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# set repo_root to the parent directory to find 'quannto' modules\n",
        "repo_root = Path.cwd().resolve().parent  \n",
        "sys.path.insert(0, str(repo_root))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "10fe6b22",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-02-18 18:36:20.766778: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2026-02-18 18:36:20.766895: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2026-02-18 18:36:20.794980: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2026-02-18 18:36:22.377796: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "from functools import partial\n",
        "import numpy as np\n",
        "\n",
        "from quannto.core.qnn_trainers import *\n",
        "from quannto.utils.path_utils import datasets_dir, models_testing_results_path, models_train_losses_path\n",
        "from quannto.utils.results_utils import *\n",
        "from quannto.core.data_processors import *\n",
        "from quannto.core.loss_functions import *\n",
        "\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61d4624e",
      "metadata": {},
      "source": [
        "## 1) Hyperparameters\n",
        "\n",
        "We train multiple CV-QONNs while sweeping the number of modes. Each configuration has:\n",
        "- 1 layer,\n",
        "- photon subtraction (default),\n",
        "- ladder-mode patterns that scale with `N`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4020f591",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === HYPERPARAMETERS DEFINITION ===\n",
        "qnns_modes = [2, 3, 4]\n",
        "qnns_ladder_modes = [[[1]], [[1, 2]], [[1, 2, 3]]]\n",
        "qnns_layers = [1, 1, 1]\n",
        "qnns_is_addition = [False, False, False]\n",
        "\n",
        "include_initial_squeezing = False\n",
        "include_initial_mixing = False\n",
        "is_passive_gaussian = False\n",
        "\n",
        "n_inputs = 1\n",
        "n_outputs = 1\n",
        "\n",
        "# Special observable for this dataset/task\n",
        "observable = 'cubicphase'\n",
        "\n",
        "# Optional normalization range for inputs (None disables rescaling)\n",
        "in_norm_ranges = [None] * len(qnns_modes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1096ef7f",
      "metadata": {},
      "source": [
        "## 2) Optimizer settings\n",
        "\n",
        "We use the hybrid trainer with MSE loss. Basinhopping is disabled here (`basinhopping_iters = 0`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c027ec3f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === OPTIMIZER SETTINGS ===\n",
        "optimize = hybrid_build_and_train_model\n",
        "loss_function = mse\n",
        "basinhopping_iters = 0\n",
        "params = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b90ed38c",
      "metadata": {},
      "source": [
        "## 3) Dataset settings (cubic-phase gate statistics)\n",
        "\n",
        "We assume a precomputed dataset exists under the QuaNNTO datasets directory:\n",
        "- `{task_name}_inputs.npy`\n",
        "- `{task_name}_outputs.npy`\n",
        "\n",
        "The dataset is labeled by the cubic-phase strength `gamma` and a set of coherent amplitudes `alpha_list`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "36b6675b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "task_name: fock_cubicphase_gamma0.2_trainsize50_rng-2.0to2.0\n",
            "dataset_dir prefix: /home/tkrasimi/QuanticGit/Quannto/datasets/fock_cubicphase_gamma0.2_trainsize50_rng-2.0to2.0\n"
          ]
        }
      ],
      "source": [
        "# === DATASET SETTINGS === (Statistics of a specific non-Gaussian gate action over target quantum states)\n",
        "gamma = 0.2\n",
        "dataset_size = 50\n",
        "input_range = (-2, 2)\n",
        "\n",
        "alpha_list = np.linspace(input_range[0], input_range[1], dataset_size)\n",
        "\n",
        "task_name = (\n",
        "    f\"fock_cubicphase_gamma{gamma}_trainsize{dataset_size}\"\n",
        "    f\"_rng{alpha_list[0]}to{alpha_list[-1]}\"\n",
        ")\n",
        "\n",
        "dataset_dir = str(datasets_dir() / task_name)\n",
        "\n",
        "print(\"task_name:\", task_name)\n",
        "print(\"dataset_dir prefix:\", dataset_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dce95d4",
      "metadata": {},
      "source": [
        "## 4) Load the training dataset\n",
        "\n",
        "If the dataset files are missing, you should generate them with the appropriate dataset generator in QuaNNTO.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a79164f6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train inputs shape: (50, 1)\n",
            "Train outputs shape: (50, 6)\n"
          ]
        }
      ],
      "source": [
        "# Training dataset of the non-Gaussian gate to be learned\n",
        "with open(f\"{dataset_dir}_inputs.npy\", \"rb\") as f:\n",
        "    inputs = np.load(f)\n",
        "with open(f\"{dataset_dir}_outputs.npy\", \"rb\") as f:\n",
        "    outputs = np.load(f)\n",
        "\n",
        "train_dataset = [inputs, outputs]\n",
        "\n",
        "print(\"Train inputs shape:\", np.array(train_dataset[0]).shape)\n",
        "print(\"Train outputs shape:\", np.array(train_dataset[1]).shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84cbcecc",
      "metadata": {},
      "source": [
        "## 5) Build, train, and evaluate models (mode sweep)\n",
        "\n",
        "For each mode count `N`, we:\n",
        "- define a unique model name,\n",
        "- build preprocessors,\n",
        "- train with the hybrid trainer,\n",
        "- evaluate on the training set (since no validation is used here),\n",
        "- save losses and outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "34383f92",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of tunable parameters: 14\n",
            "Number of terms for each trace: [1 1 1 1 1 1 1]\n",
            "\n",
            "=== Phase 1: Adam/AdamW warmup (JAX) ===\n",
            "[Adam] Epoch 0/200 | train loss = 550.327 | val loss = 550.327\n",
            "[Adam] Epoch 1/200 | train loss = 550.327 | val loss = 550.327\n",
            "[Adam] Epoch 2/200 | train loss = 503.059 | val loss = 503.059\n",
            "[Adam] Epoch 3/200 | train loss = 456.549 | val loss = 456.549\n",
            "[Adam] Epoch 4/200 | train loss = 411.988 | val loss = 411.988\n",
            "[Adam] Epoch 5/200 | train loss = 370.772 | val loss = 370.772\n",
            "[Adam] Epoch 6/200 | train loss = 334.469 | val loss = 334.469\n",
            "[Adam] Epoch 7/200 | train loss = 304.712 | val loss = 304.712\n",
            "[Adam] Epoch 8/200 | train loss = 282.976 | val loss = 282.976\n",
            "[Adam] Epoch 9/200 | train loss = 270.135 | val loss = 270.135\n",
            "[Adam] Epoch 10/200 | train loss = 265.853 | val loss = 265.853\n",
            "[Adam] Epoch 11/200 | train loss = 268.121 | val loss = 268.121\n",
            "[Adam] Epoch 12/200 | train loss = 273.511 | val loss = 273.511\n",
            "[Adam] Epoch 13/200 | train loss = 278.38 | val loss = 278.38\n",
            "[Adam] Epoch 14/200 | train loss = 280.241 | val loss = 280.241\n",
            "[Adam] Epoch 15/200 | train loss = 278.285 | val loss = 278.285\n",
            "[Adam] Epoch 16/200 | train loss = 273.073 | val loss = 273.073\n",
            "[Adam] Epoch 17/200 | train loss = 265.891 | val loss = 265.891\n",
            "[Adam] Epoch 18/200 | train loss = 258.156 | val loss = 258.156\n",
            "[Adam] Epoch 19/200 | train loss = 251.027 | val loss = 251.027\n",
            "[Adam] Epoch 20/200 | train loss = 245.225 | val loss = 245.225\n",
            "[Adam] Epoch 21/200 | train loss = 241.017 | val loss = 241.017\n",
            "[Adam] Epoch 22/200 | train loss = 238.29 | val loss = 238.29\n",
            "[Adam] Epoch 23/200 | train loss = 236.691 | val loss = 236.691\n",
            "[Adam] Epoch 24/200 | train loss = 235.755 | val loss = 235.755\n",
            "[Adam] Epoch 25/200 | train loss = 235.033 | val loss = 235.033\n",
            "[Adam] Epoch 26/200 | train loss = 234.161 | val loss = 234.161\n",
            "[Adam] Epoch 27/200 | train loss = 232.903 | val loss = 232.903\n",
            "[Adam] Epoch 28/200 | train loss = 231.153 | val loss = 231.153\n",
            "[Adam] Epoch 29/200 | train loss = 228.922 | val loss = 228.922\n",
            "[Adam] Epoch 30/200 | train loss = 226.308 | val loss = 226.308\n",
            "[Adam] Epoch 31/200 | train loss = 223.465 | val loss = 223.465\n",
            "[Adam] Epoch 32/200 | train loss = 220.568 | val loss = 220.568\n",
            "[Adam] Epoch 33/200 | train loss = 217.783 | val loss = 217.783\n",
            "[Adam] Epoch 34/200 | train loss = 215.242 | val loss = 215.242\n",
            "[Adam] Epoch 35/200 | train loss = 213.018 | val loss = 213.018\n",
            "[Adam] Epoch 36/200 | train loss = 211.122 | val loss = 211.122\n",
            "[Adam] Epoch 37/200 | train loss = 209.506 | val loss = 209.506\n",
            "[Adam] Epoch 38/200 | train loss = 208.071 | val loss = 208.071\n",
            "[Adam] Epoch 39/200 | train loss = 206.697 | val loss = 206.697\n",
            "[Adam] Epoch 40/200 | train loss = 205.27 | val loss = 205.27\n",
            "[Adam] Epoch 41/200 | train loss = 203.703 | val loss = 203.703\n",
            "[Adam] Epoch 42/200 | train loss = 201.955 | val loss = 201.955\n",
            "[Adam] Epoch 43/200 | train loss = 200.034 | val loss = 200.034\n",
            "[Adam] Epoch 44/200 | train loss = 197.986 | val loss = 197.986\n",
            "[Adam] Epoch 45/200 | train loss = 195.883 | val loss = 195.883\n",
            "[Adam] Epoch 46/200 | train loss = 193.8 | val loss = 193.8\n",
            "[Adam] Epoch 47/200 | train loss = 191.804 | val loss = 191.804\n",
            "[Adam] Epoch 48/200 | train loss = 189.94 | val loss = 189.94\n",
            "[Adam] Epoch 49/200 | train loss = 188.226 | val loss = 188.226\n",
            "[Adam] Epoch 50/200 | train loss = 186.657 | val loss = 186.657\n",
            "[Adam] Epoch 51/200 | train loss = 185.212 | val loss = 185.212\n",
            "[Adam] Epoch 52/200 | train loss = 183.857 | val loss = 183.857\n",
            "[Adam] Epoch 53/200 | train loss = 182.559 | val loss = 182.559\n",
            "[Adam] Epoch 54/200 | train loss = 181.291 | val loss = 181.291\n",
            "[Adam] Epoch 55/200 | train loss = 180.032 | val loss = 180.032\n",
            "[Adam] Epoch 56/200 | train loss = 178.776 | val loss = 178.776\n",
            "[Adam] Epoch 57/200 | train loss = 177.528 | val loss = 177.528\n",
            "[Adam] Epoch 58/200 | train loss = 176.302 | val loss = 176.302\n",
            "[Adam] Epoch 59/200 | train loss = 175.117 | val loss = 175.117\n",
            "[Adam] Epoch 60/200 | train loss = 173.993 | val loss = 173.993\n",
            "[Adam] Epoch 61/200 | train loss = 172.946 | val loss = 172.946\n",
            "[Adam] Epoch 62/200 | train loss = 171.988 | val loss = 171.988\n",
            "[Adam] Epoch 63/200 | train loss = 171.121 | val loss = 171.121\n",
            "[Adam] Epoch 64/200 | train loss = 170.341 | val loss = 170.341\n",
            "[Adam] Epoch 65/200 | train loss = 169.638 | val loss = 169.638\n",
            "[Adam] Epoch 66/200 | train loss = 168.998 | val loss = 168.998\n",
            "[Adam] Epoch 67/200 | train loss = 168.407 | val loss = 168.407\n",
            "[Adam] Epoch 68/200 | train loss = 167.852 | val loss = 167.852\n",
            "[Adam] Epoch 69/200 | train loss = 167.325 | val loss = 167.325\n",
            "[Adam] Epoch 70/200 | train loss = 166.824 | val loss = 166.824\n",
            "[Adam] Epoch 71/200 | train loss = 166.346 | val loss = 166.346\n",
            "[Adam] Epoch 72/200 | train loss = 165.893 | val loss = 165.893\n",
            "[Adam] Epoch 73/200 | train loss = 165.468 | val loss = 165.468\n",
            "[Adam] Epoch 74/200 | train loss = 165.07 | val loss = 165.07\n",
            "[Adam] Epoch 75/200 | train loss = 164.698 | val loss = 164.698\n",
            "[Adam] Epoch 76/200 | train loss = 164.35 | val loss = 164.35\n",
            "[Adam] Epoch 77/200 | train loss = 164.023 | val loss = 164.023\n",
            "[Adam] Epoch 78/200 | train loss = 163.715 | val loss = 163.715\n",
            "[Adam] Epoch 79/200 | train loss = 163.424 | val loss = 163.424\n",
            "[Adam] Epoch 80/200 | train loss = 163.149 | val loss = 163.149\n",
            "[Adam] Epoch 81/200 | train loss = 162.89 | val loss = 162.89\n",
            "[Adam] Epoch 82/200 | train loss = 162.649 | val loss = 162.649\n",
            "[Adam] Epoch 83/200 | train loss = 162.426 | val loss = 162.426\n",
            "[Adam] Epoch 84/200 | train loss = 162.221 | val loss = 162.221\n",
            "[Adam] Epoch 85/200 | train loss = 162.032 | val loss = 162.032\n",
            "[Adam] Epoch 86/200 | train loss = 161.859 | val loss = 161.859\n",
            "[Adam] Epoch 87/200 | train loss = 161.698 | val loss = 161.698\n",
            "[Adam] Epoch 88/200 | train loss = 161.546 | val loss = 161.546\n",
            "[Adam] Epoch 89/200 | train loss = 161.401 | val loss = 161.401\n",
            "[Adam] Epoch 90/200 | train loss = 161.26 | val loss = 161.26\n",
            "[Adam] Epoch 91/200 | train loss = 161.122 | val loss = 161.122\n",
            "[Adam] Epoch 92/200 | train loss = 160.985 | val loss = 160.985\n",
            "[Adam] Epoch 93/200 | train loss = 160.849 | val loss = 160.849\n",
            "[Adam] Epoch 94/200 | train loss = 160.714 | val loss = 160.714\n",
            "[Adam] Epoch 95/200 | train loss = 160.58 | val loss = 160.58\n",
            "[Adam] Epoch 96/200 | train loss = 160.447 | val loss = 160.447\n",
            "[Adam] Epoch 97/200 | train loss = 160.316 | val loss = 160.316\n",
            "[Adam] Epoch 98/200 | train loss = 160.185 | val loss = 160.185\n",
            "[Adam] Epoch 99/200 | train loss = 160.056 | val loss = 160.056\n",
            "[Adam] Epoch 100/200 | train loss = 159.927 | val loss = 159.927\n",
            "[Adam] Epoch 101/200 | train loss = 159.798 | val loss = 159.798\n",
            "[Adam] Epoch 102/200 | train loss = 159.67 | val loss = 159.67\n",
            "[Adam] Epoch 103/200 | train loss = 159.542 | val loss = 159.542\n",
            "[Adam] Epoch 104/200 | train loss = 159.414 | val loss = 159.414\n",
            "[Adam] Epoch 105/200 | train loss = 159.285 | val loss = 159.285\n",
            "[Adam] Epoch 106/200 | train loss = 159.158 | val loss = 159.158\n",
            "[Adam] Epoch 107/200 | train loss = 159.03 | val loss = 159.03\n",
            "[Adam] Epoch 108/200 | train loss = 158.902 | val loss = 158.902\n",
            "[Adam] Epoch 109/200 | train loss = 158.774 | val loss = 158.774\n",
            "[Adam] Epoch 110/200 | train loss = 158.647 | val loss = 158.647\n",
            "[Adam] Epoch 111/200 | train loss = 158.518 | val loss = 158.518\n",
            "[Adam] Epoch 112/200 | train loss = 158.39 | val loss = 158.39\n",
            "[Adam] Epoch 113/200 | train loss = 158.261 | val loss = 158.261\n",
            "[Adam] Epoch 114/200 | train loss = 158.131 | val loss = 158.131\n",
            "[Adam] Epoch 115/200 | train loss = 158 | val loss = 158\n",
            "[Adam] Epoch 116/200 | train loss = 157.869 | val loss = 157.869\n",
            "[Adam] Epoch 117/200 | train loss = 157.737 | val loss = 157.737\n",
            "[Adam] Epoch 118/200 | train loss = 157.604 | val loss = 157.604\n",
            "[Adam] Epoch 119/200 | train loss = 157.469 | val loss = 157.469\n",
            "[Adam] Epoch 120/200 | train loss = 157.333 | val loss = 157.333\n",
            "[Adam] Epoch 121/200 | train loss = 157.196 | val loss = 157.196\n",
            "[Adam] Epoch 122/200 | train loss = 157.057 | val loss = 157.057\n",
            "[Adam] Epoch 123/200 | train loss = 156.916 | val loss = 156.916\n",
            "[Adam] Epoch 124/200 | train loss = 156.774 | val loss = 156.774\n",
            "[Adam] Epoch 125/200 | train loss = 156.63 | val loss = 156.63\n",
            "[Adam] Epoch 126/200 | train loss = 156.484 | val loss = 156.484\n",
            "[Adam] Epoch 127/200 | train loss = 156.337 | val loss = 156.337\n",
            "[Adam] Epoch 128/200 | train loss = 156.188 | val loss = 156.188\n",
            "[Adam] Epoch 129/200 | train loss = 156.038 | val loss = 156.038\n",
            "[Adam] Epoch 130/200 | train loss = 155.887 | val loss = 155.887\n",
            "[Adam] Epoch 131/200 | train loss = 155.734 | val loss = 155.734\n",
            "[Adam] Epoch 132/200 | train loss = 155.58 | val loss = 155.58\n",
            "[Adam] Epoch 133/200 | train loss = 155.424 | val loss = 155.424\n",
            "[Adam] Epoch 134/200 | train loss = 155.268 | val loss = 155.268\n",
            "[Adam] Epoch 135/200 | train loss = 155.11 | val loss = 155.11\n",
            "[Adam] Epoch 136/200 | train loss = 154.952 | val loss = 154.952\n",
            "[Adam] Epoch 137/200 | train loss = 154.792 | val loss = 154.792\n",
            "[Adam] Epoch 138/200 | train loss = 154.632 | val loss = 154.632\n",
            "[Adam] Epoch 139/200 | train loss = 154.471 | val loss = 154.471\n",
            "[Adam] Epoch 140/200 | train loss = 154.31 | val loss = 154.31\n",
            "[Adam] Epoch 141/200 | train loss = 154.147 | val loss = 154.147\n",
            "[Adam] Epoch 142/200 | train loss = 153.984 | val loss = 153.984\n",
            "[Adam] Epoch 143/200 | train loss = 153.821 | val loss = 153.821\n",
            "[Adam] Epoch 144/200 | train loss = 153.658 | val loss = 153.658\n",
            "[Adam] Epoch 145/200 | train loss = 153.494 | val loss = 153.494\n",
            "[Adam] Epoch 146/200 | train loss = 153.33 | val loss = 153.33\n",
            "[Adam] Epoch 147/200 | train loss = 153.166 | val loss = 153.166\n",
            "[Adam] Epoch 148/200 | train loss = 153.002 | val loss = 153.002\n",
            "[Adam] Epoch 149/200 | train loss = 152.839 | val loss = 152.839\n",
            "[Adam] Epoch 150/200 | train loss = 152.676 | val loss = 152.676\n",
            "[Adam] Epoch 151/200 | train loss = 152.513 | val loss = 152.513\n",
            "[Adam] Epoch 152/200 | train loss = 152.351 | val loss = 152.351\n",
            "[Adam] Epoch 153/200 | train loss = 152.19 | val loss = 152.19\n",
            "[Adam] Epoch 154/200 | train loss = 152.029 | val loss = 152.029\n",
            "[Adam] Epoch 155/200 | train loss = 151.87 | val loss = 151.87\n",
            "[Adam] Epoch 156/200 | train loss = 151.711 | val loss = 151.711\n",
            "[Adam] Epoch 157/200 | train loss = 151.554 | val loss = 151.554\n",
            "[Adam] Epoch 158/200 | train loss = 151.398 | val loss = 151.398\n",
            "[Adam] Epoch 159/200 | train loss = 151.243 | val loss = 151.243\n",
            "[Adam] Epoch 160/200 | train loss = 151.09 | val loss = 151.09\n",
            "[Adam] Epoch 161/200 | train loss = 150.938 | val loss = 150.938\n",
            "[Adam] Epoch 162/200 | train loss = 150.788 | val loss = 150.788\n",
            "[Adam] Epoch 163/200 | train loss = 150.639 | val loss = 150.639\n",
            "[Adam] Epoch 164/200 | train loss = 150.492 | val loss = 150.492\n",
            "[Adam] Epoch 165/200 | train loss = 150.347 | val loss = 150.347\n",
            "[Adam] Epoch 166/200 | train loss = 150.204 | val loss = 150.204\n",
            "[Adam] Epoch 167/200 | train loss = 150.063 | val loss = 150.063\n",
            "[Adam] Epoch 168/200 | train loss = 149.923 | val loss = 149.923\n",
            "[Adam] Epoch 169/200 | train loss = 149.786 | val loss = 149.786\n",
            "[Adam] Epoch 170/200 | train loss = 149.651 | val loss = 149.651\n",
            "[Adam] Epoch 171/200 | train loss = 149.518 | val loss = 149.518\n",
            "[Adam] Epoch 172/200 | train loss = 149.387 | val loss = 149.387\n",
            "[Adam] Epoch 173/200 | train loss = 149.258 | val loss = 149.258\n",
            "[Adam] Epoch 174/200 | train loss = 149.131 | val loss = 149.131\n",
            "[Adam] Epoch 175/200 | train loss = 149.007 | val loss = 149.007\n",
            "[Adam] Epoch 176/200 | train loss = 148.884 | val loss = 148.884\n",
            "[Adam] Epoch 177/200 | train loss = 148.764 | val loss = 148.764\n",
            "[Adam] Epoch 178/200 | train loss = 148.646 | val loss = 148.646\n",
            "[Adam] Epoch 179/200 | train loss = 148.531 | val loss = 148.531\n",
            "[Adam] Epoch 180/200 | train loss = 148.417 | val loss = 148.417\n",
            "[Adam] Epoch 181/200 | train loss = 148.305 | val loss = 148.305\n",
            "[Adam] Epoch 182/200 | train loss = 148.196 | val loss = 148.196\n",
            "[Adam] Epoch 183/200 | train loss = 148.089 | val loss = 148.089\n",
            "[Adam] Epoch 184/200 | train loss = 147.983 | val loss = 147.983\n",
            "[Adam] Epoch 185/200 | train loss = 147.88 | val loss = 147.88\n",
            "[Adam] Epoch 186/200 | train loss = 147.779 | val loss = 147.779\n",
            "[Adam] Epoch 187/200 | train loss = 147.68 | val loss = 147.68\n",
            "[Adam] Epoch 188/200 | train loss = 147.583 | val loss = 147.583\n",
            "[Adam] Epoch 189/200 | train loss = 147.487 | val loss = 147.487\n",
            "[Adam] Epoch 190/200 | train loss = 147.394 | val loss = 147.394\n",
            "[Adam] Epoch 191/200 | train loss = 147.302 | val loss = 147.302\n",
            "[Adam] Epoch 192/200 | train loss = 147.213 | val loss = 147.213\n",
            "[Adam] Epoch 193/200 | train loss = 147.125 | val loss = 147.125\n",
            "[Adam] Epoch 194/200 | train loss = 147.039 | val loss = 147.039\n",
            "[Adam] Epoch 195/200 | train loss = 146.954 | val loss = 146.954\n",
            "[Adam] Epoch 196/200 | train loss = 146.871 | val loss = 146.871\n",
            "[Adam] Epoch 197/200 | train loss = 146.79 | val loss = 146.79\n",
            "[Adam] Epoch 198/200 | train loss = 146.711 | val loss = 146.711\n",
            "[Adam] Epoch 199/200 | train loss = 146.633 | val loss = 146.633\n",
            "[Adam] Epoch 200/200 | train loss = 146.556 | val loss = 146.556\n",
            "Adam warmup total time: 4.200 s\n",
            "Adam warmup time per epoch: 0.021000 s\n",
            "Best Adam train loss = 146.556, best Adam val loss = 146.556\n",
            "Training loss: 146.3947438219464\n",
            "Training loss: 146.32566471488067\n",
            "Training loss: 146.2071007274924\n",
            "Training loss: 145.7071103872814\n",
            "Training loss: 145.3608509120945\n",
            "Training loss: 144.95080779809615\n",
            "Training loss: 143.875643512157\n",
            "Training loss: 143.40938327664585\n",
            "Training loss: 143.1544097343377\n",
            "Training loss: 142.57293673172305\n",
            "Training loss: 142.390049355602\n",
            "Training loss: 141.96125480116385\n",
            "Training loss: 141.93157805288115\n",
            "Training loss: 141.91232326580442\n",
            "Training loss: 141.89656171671413\n",
            "Training loss: 141.86878112493363\n",
            "Training loss: 141.84722051101565\n",
            "Training loss: 141.82817608765936\n",
            "Training loss: 141.82128062716527\n",
            "Training loss: 141.80671508242833\n",
            "Training loss: 141.77822731387946\n",
            "Training loss: 141.73825833986538\n",
            "Training loss: 141.72140573918253\n",
            "Training loss: 141.71097766379083\n",
            "Training loss: 141.68872263577472\n",
            "Training loss: 141.68371592154213\n",
            "Training loss: 141.6770111933219\n",
            "Training loss: 141.67291784111563\n",
            "Training loss: 141.66287082278646\n",
            "Training loss: 141.64765749948435\n",
            "Training loss: 141.63221589125993\n",
            "Training loss: 141.62034920307264\n",
            "Training loss: 141.60923034730496\n",
            "Training loss: 141.59368726680555\n",
            "Training loss: 141.5828795128153\n",
            "Training loss: 141.49151569154907\n",
            "Training loss: 141.1438483263112\n",
            "Training loss: 141.03943571062905\n",
            "Training loss: 140.70231949884777\n",
            "Training loss: 140.0937800997817\n",
            "Training loss: 138.48844302565402\n",
            "Training loss: 136.49621763185772\n",
            "Training loss: 135.0978789854134\n",
            "Training loss: 133.14478082284074\n",
            "Training loss: 130.47377217558605\n",
            "Training loss: 123.46951149329844\n",
            "Training loss: 118.00473740536931\n",
            "Training loss: 116.29919868880812\n",
            "Training loss: 108.43376658839908\n",
            "Training loss: 105.01996582542739\n",
            "Training loss: 101.87339538175311\n",
            "Training loss: 99.68736077496641\n",
            "Training loss: 97.98817806962371\n",
            "Training loss: 96.23248125328055\n",
            "Training loss: 95.06775917674265\n",
            "Training loss: 94.36774850537778\n",
            "Training loss: 93.47865925381855\n",
            "Training loss: 93.28266893821477\n",
            "Training loss: 93.11681804732767\n",
            "Training loss: 92.92820801153219\n",
            "Training loss: 92.63919921614252\n",
            "Training loss: 92.47987171963047\n",
            "Training loss: 92.37387772294464\n",
            "Training loss: 92.2579151883648\n",
            "Training loss: 92.15680440540487\n",
            "Training loss: 91.99190668945091\n",
            "Training loss: 91.80047958259867\n",
            "Training loss: 91.4559855484026\n",
            "Training loss: 91.1335463919133\n",
            "Training loss: 90.70503478107156\n",
            "Training loss: 90.21762194193327\n",
            "Training loss: 89.4427493895969\n",
            "Training loss: 87.46212652329352\n",
            "Training loss: 86.71093594956146\n",
            "Training loss: 85.71038343210624\n",
            "Training loss: 84.72745555415007\n",
            "Training loss: 84.25469831954055\n",
            "Training loss: 83.64694326760655\n",
            "Training loss: 83.19113667206217\n",
            "Training loss: 82.85938539982614\n",
            "Training loss: 82.63257820990916\n",
            "Training loss: 82.47969241305131\n",
            "Training loss: 82.39927143249203\n",
            "Training loss: 82.37505933675308\n",
            "Training loss: 82.27315051197544\n",
            "Training loss: 81.99472456912675\n",
            "Training loss: 81.5041486959431\n",
            "Training loss: 81.29493706774007\n",
            "Training loss: 80.4816310357951\n",
            "Training loss: 80.08421432641475\n",
            "Training loss: 79.71934549691879\n",
            "Training loss: 79.57368007219267\n",
            "Training loss: 79.34214982809368\n",
            "Training loss: 79.17811040428954\n",
            "Training loss: 78.6806560141247\n",
            "Training loss: 78.43763653930102\n",
            "Training loss: 78.22512953163405\n",
            "Training loss: 78.08912006328755\n",
            "Training loss: 77.99304157974157\n",
            "Training loss: 77.96275304903742\n",
            "Training loss: 77.92347582402066\n",
            "Training loss: 77.81399084417896\n",
            "Training loss: 77.76872187335562\n",
            "Training loss: 77.7157446610346\n",
            "Training loss: 77.50919259410546\n",
            "Training loss: 77.30377794866902\n",
            "Training loss: 77.24011373981945\n",
            "Training loss: 77.10529476535217\n",
            "Training loss: 77.0360392939676\n",
            "Training loss: 76.98271499582448\n",
            "Training loss: 76.96884118841676\n",
            "Training loss: 76.8993103664486\n",
            "Training loss: 76.84402113438884\n",
            "Training loss: 76.74402138170115\n",
            "Training loss: 76.59820584041123\n",
            "Training loss: 76.2624684498104\n",
            "Training loss: 75.99409368801676\n",
            "Training loss: 75.57745595069228\n",
            "Training loss: 75.43087623679392\n",
            "Training loss: 75.38991547244343\n",
            "Training loss: 75.31253811949402\n",
            "Training loss: 75.27898360610618\n",
            "Training loss: 75.20697195148023\n",
            "Training loss: 74.78064002000349\n",
            "Training loss: 74.318321018547\n",
            "Training loss: 73.88108209688251\n",
            "Training loss: 73.71010997942629\n",
            "Training loss: 73.64139725512807\n",
            "Training loss: 73.58967429448596\n",
            "Training loss: 73.56687029999829\n",
            "Training loss: 73.54193285600093\n",
            "Training loss: 73.5235029193649\n",
            "Training loss: 73.48464115063402\n",
            "Training loss: 73.32842675108947\n",
            "Training loss: 73.08931894529216\n",
            "Training loss: 73.02606404159027\n",
            "Training loss: 73.00823753484107\n",
            "Training loss: 73.00120609248505\n",
            "Training loss: 72.98346643553084\n",
            "Training loss: 72.94994537217495\n",
            "Training loss: 72.8846128584896\n",
            "Training loss: 72.8655125462479\n",
            "Training loss: 72.78433337198808\n",
            "Training loss: 72.64622443369593\n",
            "Training loss: 72.62749142273493\n",
            "Training loss: 72.61897347488299\n",
            "Training loss: 72.61514849082224\n",
            "Training loss: 72.6067079700272\n",
            "Training loss: 72.59789506333772\n",
            "Training loss: 72.58852071430815\n",
            "Training loss: 72.57798717163048\n",
            "Training loss: 72.55847680696766\n",
            "Training loss: 72.54919800932349\n",
            "Training loss: 72.54024278919451\n",
            "Training loss: 72.53108742291721\n",
            "Training loss: 72.52943019511062\n",
            "Training loss: 72.52754986428772\n",
            "Training loss: 72.52230776498715\n",
            "Training loss: 72.51944152455665\n",
            "Training loss: 72.51735602757348\n",
            "Training loss: 72.51544577336948\n",
            "Training loss: 72.51149501576681\n",
            "Training loss: 72.50209896474645\n",
            "Training loss: 72.4978141180733\n",
            "Training loss: 72.47847006465787\n",
            "Training loss: 72.46357521090292\n",
            "Training loss: 72.40909135514474\n",
            "Training loss: 72.39688860156825\n",
            "Training loss: 72.30923559701218\n",
            "Training loss: 72.22315094691719\n",
            "Training loss: 72.15514963656945\n",
            "Training loss: 72.13215309186863\n",
            "Training loss: 72.1054308866429\n",
            "Training loss: 72.08001508230464\n",
            "Training loss: 72.06068857425218\n",
            "Training loss: 72.03145553665125\n",
            "Training loss: 71.95042324594424\n",
            "Training loss: 71.83360955482844\n",
            "Training loss: 71.78307284536419\n",
            "Training loss: 71.67898087542709\n",
            "Training loss: 71.64085872624634\n",
            "Training loss: 71.6349831563262\n",
            "Training loss: 71.6296920935512\n",
            "Training loss: 71.62616345169467\n",
            "Training loss: 71.61518788525298\n",
            "Training loss: 71.59998666655495\n",
            "Training loss: 71.59338050126092\n",
            "Training loss: 71.58397761796125\n",
            "Training loss: 71.57316941523474\n",
            "Training loss: 71.5627285946918\n",
            "Training loss: 71.56032949656594\n",
            "Training loss: 71.5550945834559\n",
            "Training loss: 71.55257179928586\n",
            "Training loss: 71.54916851934912\n",
            "Training loss: 71.54308130645468\n",
            "Training loss: 71.53026774497606\n",
            "Training loss: 71.51049590633104\n",
            "Training loss: 71.47685817449519\n",
            "Training loss: 71.45448168653922\n",
            "Training loss: 71.43313107543139\n",
            "Training loss: 71.41730036525298\n",
            "Training loss: 71.39603424856598\n",
            "Training loss: 71.35688379046144\n",
            "Training loss: 71.32788191003424\n",
            "Training loss: 71.3183834568433\n",
            "Training loss: 71.2948436777234\n",
            "Training loss: 71.27711188168833\n",
            "Training loss: 71.27392043932895\n",
            "Training loss: 71.26534437857799\n",
            "Training loss: 71.25218906588883\n",
            "Training loss: 71.22942853585714\n",
            "Training loss: 71.21919269394597\n",
            "Training loss: 71.21613180446981\n",
            "Training loss: 71.21432860241525\n",
            "Training loss: 71.21173778471353\n",
            "Training loss: 71.20774872773876\n",
            "Training loss: 71.20436988006985\n",
            "Training loss: 71.2012277751101\n",
            "Training loss: 71.19833531305294\n",
            "Training loss: 71.19473442728263\n",
            "Training loss: 71.18948725259716\n",
            "Training loss: 71.17950638785368\n",
            "Training loss: 71.16954343977086\n",
            "Training loss: 71.16238678900051\n",
            "Training loss: 71.14748139809352\n",
            "Training loss: 71.13718971472723\n",
            "Training loss: 71.13291503344558\n",
            "Training loss: 71.1287421040328\n",
            "Training loss: 71.1247749340975\n",
            "Training loss: 71.12081272037094\n",
            "Training loss: 71.11761066960842\n",
            "Training loss: 71.10336484479707\n",
            "Training loss: 71.08747036509817\n",
            "Training loss: 71.061045046214\n",
            "Training loss: 71.0331008615296\n",
            "Training loss: 71.02655920727226\n",
            "Training loss: 71.0019453522989\n",
            "Training loss: 70.9852110718663\n",
            "Training loss: 70.96678783126751\n",
            "Training loss: 70.94954186802153\n",
            "Training loss: 70.94324774895098\n",
            "Training loss: 70.93778771459847\n",
            "Training loss: 70.9359645572358\n",
            "Training loss: 70.93315112010877\n",
            "Training loss: 70.93137176085523\n",
            "Training loss: 70.92802027399186\n",
            "Training loss: 70.92098472328468\n",
            "Training loss: 70.91838838837158\n",
            "Training loss: 70.91436958666411\n",
            "Training loss: 70.91360530289482\n",
            "Training loss: 70.91328995495795\n",
            "Training loss: 70.91310851299114\n",
            "Training loss: 70.91304579967922\n",
            "Training loss: 70.9128747750939\n",
            "Training loss: 70.91264346964554\n",
            "Training loss: 70.91246851473763\n",
            "Training loss: 70.91238480970436\n",
            "Training loss: 70.91208851374255\n",
            "Training loss: 70.91188275228149\n",
            "Training loss: 70.9116264566889\n",
            "Training loss: 70.91156607741239\n",
            "Training loss: 70.91142516023828\n",
            "Training loss: 70.91137304124086\n",
            "Training loss: 70.91125345200123\n",
            "Training loss: 70.91104583045694\n",
            "Training loss: 70.91053670814138\n",
            "Training loss: 70.91023061506141\n",
            "Training loss: 70.90993549509092\n",
            "Training loss: 70.90907629303408\n",
            "Training loss: 70.9085768639291\n",
            "Training loss: 70.90817049542932\n",
            "Training loss: 70.90746543181618\n",
            "Training loss: 70.90627877737649\n",
            "Training loss: 70.90586089027785\n",
            "Training loss: 70.90571535923925\n",
            "Training loss: 70.90313675686916\n",
            "Training loss: 70.900237069544\n",
            "Training loss: 70.89952781934886\n",
            "Training loss: 70.89632924447218\n",
            "Training loss: 70.89463678042704\n",
            "Training loss: 70.89284708403245\n",
            "Training loss: 70.8915478747121\n",
            "Training loss: 70.88913163773425\n",
            "Training loss: 70.88778091474151\n",
            "Training loss: 70.88601461477117\n",
            "Training loss: 70.88565792003055\n",
            "Training loss: 70.88511030962265\n",
            "Training loss: 70.8849686802781\n",
            "Training loss: 70.88491368514185\n",
            "Training loss: 70.88473200543545\n",
            "Training loss: 70.88459422720777\n",
            "Training loss: 70.88447902522549\n",
            "Training loss: 70.8844421104153\n",
            "Training loss: 70.88434769516374\n",
            "Training loss: 70.88428814958915\n",
            "Training loss: 70.88417707974556\n",
            "Training loss: 70.88401843285031\n",
            "Training loss: 70.88331036060788\n",
            "Training loss: 70.88320045278344\n",
            "Training loss: 70.88294191724343\n",
            "Training loss: 70.8826527408856\n",
            "Training loss: 70.8825054688423\n",
            "Training loss: 70.88232786890518\n",
            "Training loss: 70.88201977446853\n",
            "Training loss: 70.88098860001207\n",
            "Training loss: 70.87970423362387\n",
            "Training loss: 70.87785398846958\n",
            "Training loss: 70.8778358314319\n",
            "Training loss: 70.87586558665363\n",
            "Training loss: 70.8747095519231\n",
            "Training loss: 70.87375032111852\n",
            "Training loss: 70.87142192802938\n",
            "Training loss: 70.8707077310609\n",
            "Training loss: 70.87062760437846\n",
            "Training loss: 70.87058858771779\n",
            "Training loss: 70.87053374585163\n",
            "Training loss: 70.87042575801391\n",
            "Training loss: 70.8704031926407\n",
            "Training loss: 70.87032303200465\n",
            "Training loss: 70.87022570277368\n",
            "Training loss: 70.8699799978165\n",
            "Training loss: 70.86978232322025\n",
            "Training loss: 70.86977239983095\n",
            "Training loss: 70.8697371427759\n",
            "Training loss: 70.8697174188864\n",
            "Training loss: 70.86970364405379\n",
            "Training loss: 70.8697017321105\n",
            "Training loss: 70.86970173203906\n",
            "Best basinhopping iteration error so far: 9999\n",
            "Current basinhopping iteration error: 70.86970173203906\n",
            "==========\n",
            "\n",
            "Total training time: 12.676356554031372 seconds\n",
            "Time per epoch: 0.03864742851838833 seconds\n",
            "\n",
            "OPTIMIZATION ERROR FOR N=2, L=1, â modes=[2]\n",
            "70.86970173203906\n",
            "PARAMETERS:\n",
            "[-3.11773957e-02 -3.57577091e-02  3.41092488e-01  9.10477480e-01\n",
            "  4.44208717e-02  6.75846390e-03 -1.41337528e-01 -1.71441026e-01\n",
            "  2.03406428e-01  4.81806659e-04  1.71031351e-01  1.87437125e-02\n",
            "  1.46143253e+00  2.15224189e-02]\n",
            "=== ACTIVE LAYER 1 ===\n",
            "Q1 = [[ 0.9414281 +0.j  0.04659023+0.j  0.33395254+0.j -0.0042651 +0.j]\n",
            " [-0.01057189+0.j  0.61253111+0.j -0.04557495+0.j  0.78906071+0.j]\n",
            " [-0.33395254+0.j  0.0042651 +0.j  0.9414281 +0.j  0.04659023+0.j]\n",
            " [ 0.04557495+0.j -0.78906071+0.j -0.01057189+0.j  0.61253111+0.j]]\n",
            "Z = [[1.22557047+0.j 0.        +0.j 0.        +0.j 0.        +0.j]\n",
            " [0.        +0.j 1.00048192+0.j 0.        +0.j 0.        +0.j]\n",
            " [0.        +0.j 0.        +0.j 0.81594655+0.j 0.        +0.j]\n",
            " [0.        +0.j 0.        +0.j 0.        +0.j 0.99951831+0.j]]\n",
            "Q2 = [[ 9.89030758e-01+0.j  2.42589918e-04+0.j -1.40715242e-01+0.j\n",
            "   4.49145987e-02+0.j]\n",
            " [ 1.35895622e-02+0.j  9.84343815e-01+0.j  4.28100903e-02+0.j\n",
            "  -1.70440233e-01+0.j]\n",
            " [ 1.40715242e-01+0.j -4.49145987e-02+0.j  9.89030758e-01+0.j\n",
            "   2.42589918e-04+0.j]\n",
            " [-4.28100903e-02+0.j  1.70440233e-01+0.j  1.35895622e-02+0.j\n",
            "   9.84343815e-01+0.j]]\n",
            "Symplectic matrix:\n",
            "[[ 1.18151688+0.j  0.02070896+0.j  0.29621607+0.j  0.01717069+0.j]\n",
            " [-0.01416121+0.j  0.73857963+0.j -0.00463528+0.j  0.67428808+0.j]\n",
            " [-0.10665699+0.j -0.01623947+0.j  0.8193666 +0.j  0.00155369+0.j]\n",
            " [-0.01005962+0.j -0.67427978+0.j -0.02525543+0.j  0.73794367+0.j]]\n",
            "Symplecticity condition: [[ 0.00000000e+00+0.j  1.73472348e-17+0.j]\n",
            " [-1.73472348e-17+0.j  0.00000000e+00+0.j]]\n",
            "True\n",
            "Orthogonality condition: [[1.48444979+0.j 0.00876846+0.j]\n",
            " [0.00876846+0.j 1.00038631+0.j]]\n",
            "False\n",
            "\n",
            "Displacement vector:\n",
            "[0.17103135+0.j 0.01874371+0.j 1.46143253+0.j 0.02152242+0.j]\n",
            "Trained model: fock_cubicphase_gamma0.2_trainsize50_rng-2.0to2.0_N2_L1_sub[[1]]_inNone\n",
            "Final training loss: 70.86970173203906\n",
            "Number of tunable parameters: 27\n",
            "Number of terms for each trace: [1 1 1 1 1 1 1]\n",
            "\n",
            "=== Phase 1: Adam/AdamW warmup (JAX) ===\n",
            "[Adam] Epoch 0/200 | train loss = 640.015 | val loss = 640.015\n",
            "[Adam] Epoch 1/200 | train loss = 640.015 | val loss = 640.015\n",
            "[Adam] Epoch 2/200 | train loss = 584.502 | val loss = 584.502\n",
            "[Adam] Epoch 3/200 | train loss = 526.631 | val loss = 526.631\n",
            "[Adam] Epoch 4/200 | train loss = 467.457 | val loss = 467.457\n",
            "[Adam] Epoch 5/200 | train loss = 410.13 | val loss = 410.13\n",
            "[Adam] Epoch 6/200 | train loss = 359.191 | val loss = 359.191\n",
            "[Adam] Epoch 7/200 | train loss = 320.266 | val loss = 320.266\n",
            "[Adam] Epoch 8/200 | train loss = 298.077 | val loss = 298.077\n",
            "[Adam] Epoch 9/200 | train loss = 293.046 | val loss = 293.046\n",
            "[Adam] Epoch 10/200 | train loss = 298.654 | val loss = 298.654\n",
            "[Adam] Epoch 11/200 | train loss = 302.494 | val loss = 302.494\n",
            "[Adam] Epoch 12/200 | train loss = 295.542 | val loss = 295.542\n",
            "[Adam] Epoch 13/200 | train loss = 277.499 | val loss = 277.499\n",
            "[Adam] Epoch 14/200 | train loss = 253.922 | val loss = 253.922\n",
            "[Adam] Epoch 15/200 | train loss = 231.782 | val loss = 231.782\n",
            "[Adam] Epoch 16/200 | train loss = 216.665 | val loss = 216.665\n",
            "[Adam] Epoch 17/200 | train loss = 211.118 | val loss = 211.118\n",
            "[Adam] Epoch 18/200 | train loss = 213.289 | val loss = 213.289\n",
            "[Adam] Epoch 19/200 | train loss = 217.638 | val loss = 217.638\n",
            "[Adam] Epoch 20/200 | train loss = 218.845 | val loss = 218.845\n",
            "[Adam] Epoch 21/200 | train loss = 214.663 | val loss = 214.663\n",
            "[Adam] Epoch 22/200 | train loss = 205.791 | val loss = 205.791\n",
            "[Adam] Epoch 23/200 | train loss = 194.851 | val loss = 194.851\n",
            "[Adam] Epoch 24/200 | train loss = 185.257 | val loss = 185.257\n",
            "[Adam] Epoch 25/200 | train loss = 179.663 | val loss = 179.663\n",
            "[Adam] Epoch 26/200 | train loss = 178.347 | val loss = 178.347\n",
            "[Adam] Epoch 27/200 | train loss = 179.004 | val loss = 179.004\n",
            "[Adam] Epoch 28/200 | train loss = 178.626 | val loss = 178.626\n",
            "[Adam] Epoch 29/200 | train loss = 175.722 | val loss = 175.722\n",
            "[Adam] Epoch 30/200 | train loss = 170.916 | val loss = 170.916\n",
            "[Adam] Epoch 31/200 | train loss = 165.969 | val loss = 165.969\n",
            "[Adam] Epoch 32/200 | train loss = 162.369 | val loss = 162.369\n",
            "[Adam] Epoch 33/200 | train loss = 160.483 | val loss = 160.483\n",
            "[Adam] Epoch 34/200 | train loss = 159.625 | val loss = 159.625\n",
            "[Adam] Epoch 35/200 | train loss = 158.718 | val loss = 158.718\n",
            "[Adam] Epoch 36/200 | train loss = 156.977 | val loss = 156.977\n",
            "[Adam] Epoch 37/200 | train loss = 154.267 | val loss = 154.267\n",
            "[Adam] Epoch 38/200 | train loss = 151.101 | val loss = 151.101\n",
            "[Adam] Epoch 39/200 | train loss = 148.3 | val loss = 148.3\n",
            "[Adam] Epoch 40/200 | train loss = 146.457 | val loss = 146.457\n",
            "[Adam] Epoch 41/200 | train loss = 145.491 | val loss = 145.491\n",
            "[Adam] Epoch 42/200 | train loss = 144.697 | val loss = 144.697\n",
            "[Adam] Epoch 43/200 | train loss = 143.324 | val loss = 143.324\n",
            "[Adam] Epoch 44/200 | train loss = 141.153 | val loss = 141.153\n",
            "[Adam] Epoch 45/200 | train loss = 138.6 | val loss = 138.6\n",
            "[Adam] Epoch 46/200 | train loss = 136.336 | val loss = 136.336\n",
            "[Adam] Epoch 47/200 | train loss = 134.783 | val loss = 134.783\n",
            "[Adam] Epoch 48/200 | train loss = 133.85 | val loss = 133.85\n",
            "[Adam] Epoch 49/200 | train loss = 133.063 | val loss = 133.063\n",
            "[Adam] Epoch 50/200 | train loss = 131.933 | val loss = 131.933\n",
            "[Adam] Epoch 51/200 | train loss = 130.281 | val loss = 130.281\n",
            "[Adam] Epoch 52/200 | train loss = 128.301 | val loss = 128.301\n",
            "[Adam] Epoch 53/200 | train loss = 126.381 | val loss = 126.381\n",
            "[Adam] Epoch 54/200 | train loss = 124.81 | val loss = 124.81\n",
            "[Adam] Epoch 55/200 | train loss = 123.574 | val loss = 123.574\n",
            "[Adam] Epoch 56/200 | train loss = 122.386 | val loss = 122.386\n",
            "[Adam] Epoch 57/200 | train loss = 120.939 | val loss = 120.939\n",
            "[Adam] Epoch 58/200 | train loss = 119.157 | val loss = 119.157\n",
            "[Adam] Epoch 59/200 | train loss = 117.226 | val loss = 117.226\n",
            "[Adam] Epoch 60/200 | train loss = 115.395 | val loss = 115.395\n",
            "[Adam] Epoch 61/200 | train loss = 113.757 | val loss = 113.757\n",
            "[Adam] Epoch 62/200 | train loss = 112.178 | val loss = 112.178\n",
            "[Adam] Epoch 63/200 | train loss = 110.435 | val loss = 110.435\n",
            "[Adam] Epoch 64/200 | train loss = 108.414 | val loss = 108.414\n",
            "[Adam] Epoch 65/200 | train loss = 106.194 | val loss = 106.194\n",
            "[Adam] Epoch 66/200 | train loss = 103.947 | val loss = 103.947\n",
            "[Adam] Epoch 67/200 | train loss = 101.745 | val loss = 101.745\n",
            "[Adam] Epoch 68/200 | train loss = 99.4817 | val loss = 99.4817\n",
            "[Adam] Epoch 69/200 | train loss = 96.9857 | val loss = 96.9857\n",
            "[Adam] Epoch 70/200 | train loss = 94.2053 | val loss = 94.2053\n",
            "[Adam] Epoch 71/200 | train loss = 91.2397 | val loss = 91.2397\n",
            "[Adam] Epoch 72/200 | train loss = 88.1976 | val loss = 88.1976\n",
            "[Adam] Epoch 73/200 | train loss = 85.058 | val loss = 85.058\n",
            "[Adam] Epoch 74/200 | train loss = 81.6998 | val loss = 81.6998\n",
            "[Adam] Epoch 75/200 | train loss = 78.0676 | val loss = 78.0676\n",
            "[Adam] Epoch 76/200 | train loss = 74.2804 | val loss = 74.2804\n",
            "[Adam] Epoch 77/200 | train loss = 70.5645 | val loss = 70.5645\n",
            "[Adam] Epoch 78/200 | train loss = 67.1198 | val loss = 67.1198\n",
            "[Adam] Epoch 79/200 | train loss = 64.1064 | val loss = 64.1064\n",
            "[Adam] Epoch 80/200 | train loss = 61.7235 | val loss = 61.7235\n",
            "[Adam] Epoch 81/200 | train loss = 60.1659 | val loss = 60.1659\n",
            "[Adam] Epoch 82/200 | train loss = 59.4626 | val loss = 59.4626\n",
            "[Adam] Epoch 83/200 | train loss = 59.4561 | val loss = 59.4561\n",
            "[Adam] Epoch 84/200 | train loss = 59.905 | val loss = 59.905\n",
            "[Adam] Epoch 85/200 | train loss = 60.471 | val loss = 60.471\n",
            "[Adam] Epoch 86/200 | train loss = 60.7154 | val loss = 60.7154\n",
            "[Adam] Epoch 87/200 | train loss = 60.3197 | val loss = 60.3197\n",
            "[Adam] Epoch 88/200 | train loss = 59.281 | val loss = 59.281\n",
            "[Adam] Epoch 89/200 | train loss = 57.8256 | val loss = 57.8256\n",
            "[Adam] Epoch 90/200 | train loss = 56.2204 | val loss = 56.2204\n",
            "[Adam] Epoch 91/200 | train loss = 54.7047 | val loss = 54.7047\n",
            "[Adam] Epoch 92/200 | train loss = 53.4633 | val loss = 53.4633\n",
            "[Adam] Epoch 93/200 | train loss = 52.5516 | val loss = 52.5516\n",
            "[Adam] Epoch 94/200 | train loss = 51.8825 | val loss = 51.8825\n",
            "[Adam] Epoch 95/200 | train loss = 51.3415 | val loss = 51.3415\n",
            "[Adam] Epoch 96/200 | train loss = 50.8742 | val loss = 50.8742\n",
            "[Adam] Epoch 97/200 | train loss = 50.4408 | val loss = 50.4408\n",
            "[Adam] Epoch 98/200 | train loss = 49.9783 | val loss = 49.9783\n",
            "[Adam] Epoch 99/200 | train loss = 49.4558 | val loss = 49.4558\n",
            "[Adam] Epoch 100/200 | train loss = 48.898 | val loss = 48.898\n",
            "[Adam] Epoch 101/200 | train loss = 48.3233 | val loss = 48.3233\n",
            "[Adam] Epoch 102/200 | train loss = 47.7165 | val loss = 47.7165\n",
            "[Adam] Epoch 103/200 | train loss = 47.0802 | val loss = 47.0802\n",
            "[Adam] Epoch 104/200 | train loss = 46.4506 | val loss = 46.4506\n",
            "[Adam] Epoch 105/200 | train loss = 45.8486 | val loss = 45.8486\n",
            "[Adam] Epoch 106/200 | train loss = 45.2711 | val loss = 45.2711\n",
            "[Adam] Epoch 107/200 | train loss = 44.7278 | val loss = 44.7278\n",
            "[Adam] Epoch 108/200 | train loss = 44.2337 | val loss = 44.2337\n",
            "[Adam] Epoch 109/200 | train loss = 43.776 | val loss = 43.776\n",
            "[Adam] Epoch 110/200 | train loss = 43.3335 | val loss = 43.3335\n",
            "[Adam] Epoch 111/200 | train loss = 42.9052 | val loss = 42.9052\n",
            "[Adam] Epoch 112/200 | train loss = 42.4894 | val loss = 42.4894\n",
            "[Adam] Epoch 113/200 | train loss = 42.0695 | val loss = 42.0695\n",
            "[Adam] Epoch 114/200 | train loss = 41.6404 | val loss = 41.6404\n",
            "[Adam] Epoch 115/200 | train loss = 41.2085 | val loss = 41.2085\n",
            "[Adam] Epoch 116/200 | train loss = 40.7706 | val loss = 40.7706\n",
            "[Adam] Epoch 117/200 | train loss = 40.3244 | val loss = 40.3244\n",
            "[Adam] Epoch 118/200 | train loss = 39.8801 | val loss = 39.8801\n",
            "[Adam] Epoch 119/200 | train loss = 39.4447 | val loss = 39.4447\n",
            "[Adam] Epoch 120/200 | train loss = 39.0164 | val loss = 39.0164\n",
            "[Adam] Epoch 121/200 | train loss = 38.5982 | val loss = 38.5982\n",
            "[Adam] Epoch 122/200 | train loss = 38.1938 | val loss = 38.1938\n",
            "[Adam] Epoch 123/200 | train loss = 37.8 | val loss = 37.8\n",
            "[Adam] Epoch 124/200 | train loss = 37.4154 | val loss = 37.4154\n",
            "[Adam] Epoch 125/200 | train loss = 37.0416 | val loss = 37.0416\n",
            "[Adam] Epoch 126/200 | train loss = 36.6744 | val loss = 36.6744\n",
            "[Adam] Epoch 127/200 | train loss = 36.3082 | val loss = 36.3082\n",
            "[Adam] Epoch 128/200 | train loss = 35.9422 | val loss = 35.9422\n",
            "[Adam] Epoch 129/200 | train loss = 35.5747 | val loss = 35.5747\n",
            "[Adam] Epoch 130/200 | train loss = 35.2032 | val loss = 35.2032\n",
            "[Adam] Epoch 131/200 | train loss = 34.8303 | val loss = 34.8303\n",
            "[Adam] Epoch 132/200 | train loss = 34.4586 | val loss = 34.4586\n",
            "[Adam] Epoch 133/200 | train loss = 34.0884 | val loss = 34.0884\n",
            "[Adam] Epoch 134/200 | train loss = 33.722 | val loss = 33.722\n",
            "[Adam] Epoch 135/200 | train loss = 33.3619 | val loss = 33.3619\n",
            "[Adam] Epoch 136/200 | train loss = 33.008 | val loss = 33.008\n",
            "[Adam] Epoch 137/200 | train loss = 32.6603 | val loss = 32.6603\n",
            "[Adam] Epoch 138/200 | train loss = 32.3189 | val loss = 32.3189\n",
            "[Adam] Epoch 139/200 | train loss = 31.9815 | val loss = 31.9815\n",
            "[Adam] Epoch 140/200 | train loss = 31.6464 | val loss = 31.6464\n",
            "[Adam] Epoch 141/200 | train loss = 31.3136 | val loss = 31.3136\n",
            "[Adam] Epoch 142/200 | train loss = 30.982 | val loss = 30.982\n",
            "[Adam] Epoch 143/200 | train loss = 30.6519 | val loss = 30.6519\n",
            "[Adam] Epoch 144/200 | train loss = 30.3242 | val loss = 30.3242\n",
            "[Adam] Epoch 145/200 | train loss = 29.999 | val loss = 29.999\n",
            "[Adam] Epoch 146/200 | train loss = 29.677 | val loss = 29.677\n",
            "[Adam] Epoch 147/200 | train loss = 29.3591 | val loss = 29.3591\n",
            "[Adam] Epoch 148/200 | train loss = 29.0455 | val loss = 29.0455\n",
            "[Adam] Epoch 149/200 | train loss = 28.7361 | val loss = 28.7361\n",
            "[Adam] Epoch 150/200 | train loss = 28.4311 | val loss = 28.4311\n",
            "[Adam] Epoch 151/200 | train loss = 28.1299 | val loss = 28.1299\n",
            "[Adam] Epoch 152/200 | train loss = 27.8322 | val loss = 27.8322\n",
            "[Adam] Epoch 153/200 | train loss = 27.5381 | val loss = 27.5381\n",
            "[Adam] Epoch 154/200 | train loss = 27.2475 | val loss = 27.2475\n",
            "[Adam] Epoch 155/200 | train loss = 26.9608 | val loss = 26.9608\n",
            "[Adam] Epoch 156/200 | train loss = 26.6781 | val loss = 26.6781\n",
            "[Adam] Epoch 157/200 | train loss = 26.3995 | val loss = 26.3995\n",
            "[Adam] Epoch 158/200 | train loss = 26.1254 | val loss = 26.1254\n",
            "[Adam] Epoch 159/200 | train loss = 25.8561 | val loss = 25.8561\n",
            "[Adam] Epoch 160/200 | train loss = 25.5915 | val loss = 25.5915\n",
            "[Adam] Epoch 161/200 | train loss = 25.3319 | val loss = 25.3319\n",
            "[Adam] Epoch 162/200 | train loss = 25.0771 | val loss = 25.0771\n",
            "[Adam] Epoch 163/200 | train loss = 24.8272 | val loss = 24.8272\n",
            "[Adam] Epoch 164/200 | train loss = 24.5824 | val loss = 24.5824\n",
            "[Adam] Epoch 165/200 | train loss = 24.3427 | val loss = 24.3427\n",
            "[Adam] Epoch 166/200 | train loss = 24.1081 | val loss = 24.1081\n",
            "[Adam] Epoch 167/200 | train loss = 23.8789 | val loss = 23.8789\n",
            "[Adam] Epoch 168/200 | train loss = 23.6551 | val loss = 23.6551\n",
            "[Adam] Epoch 169/200 | train loss = 23.4367 | val loss = 23.4367\n",
            "[Adam] Epoch 170/200 | train loss = 23.2239 | val loss = 23.2239\n",
            "[Adam] Epoch 171/200 | train loss = 23.0165 | val loss = 23.0165\n",
            "[Adam] Epoch 172/200 | train loss = 22.8145 | val loss = 22.8145\n",
            "[Adam] Epoch 173/200 | train loss = 22.6181 | val loss = 22.6181\n",
            "[Adam] Epoch 174/200 | train loss = 22.427 | val loss = 22.427\n",
            "[Adam] Epoch 175/200 | train loss = 22.2413 | val loss = 22.2413\n",
            "[Adam] Epoch 176/200 | train loss = 22.0609 | val loss = 22.0609\n",
            "[Adam] Epoch 177/200 | train loss = 21.8857 | val loss = 21.8857\n",
            "[Adam] Epoch 178/200 | train loss = 21.7158 | val loss = 21.7158\n",
            "[Adam] Epoch 179/200 | train loss = 21.5509 | val loss = 21.5509\n",
            "[Adam] Epoch 180/200 | train loss = 21.391 | val loss = 21.391\n",
            "[Adam] Epoch 181/200 | train loss = 21.2359 | val loss = 21.2359\n",
            "[Adam] Epoch 182/200 | train loss = 21.0854 | val loss = 21.0854\n",
            "[Adam] Epoch 183/200 | train loss = 20.9395 | val loss = 20.9395\n",
            "[Adam] Epoch 184/200 | train loss = 20.7978 | val loss = 20.7978\n",
            "[Adam] Epoch 185/200 | train loss = 20.6602 | val loss = 20.6602\n",
            "[Adam] Epoch 186/200 | train loss = 20.5266 | val loss = 20.5266\n",
            "[Adam] Epoch 187/200 | train loss = 20.3967 | val loss = 20.3967\n",
            "[Adam] Epoch 188/200 | train loss = 20.2704 | val loss = 20.2704\n",
            "[Adam] Epoch 189/200 | train loss = 20.1475 | val loss = 20.1475\n",
            "[Adam] Epoch 190/200 | train loss = 20.0278 | val loss = 20.0278\n",
            "[Adam] Epoch 191/200 | train loss = 19.9112 | val loss = 19.9112\n",
            "[Adam] Epoch 192/200 | train loss = 19.7974 | val loss = 19.7974\n",
            "[Adam] Epoch 193/200 | train loss = 19.6863 | val loss = 19.6863\n",
            "[Adam] Epoch 194/200 | train loss = 19.5778 | val loss = 19.5778\n",
            "[Adam] Epoch 195/200 | train loss = 19.4716 | val loss = 19.4716\n",
            "[Adam] Epoch 196/200 | train loss = 19.3677 | val loss = 19.3677\n",
            "[Adam] Epoch 197/200 | train loss = 19.2659 | val loss = 19.2659\n",
            "[Adam] Epoch 198/200 | train loss = 19.166 | val loss = 19.166\n",
            "[Adam] Epoch 199/200 | train loss = 19.0681 | val loss = 19.0681\n",
            "[Adam] Epoch 200/200 | train loss = 18.972 | val loss = 18.972\n",
            "Adam warmup total time: 65.932 s\n",
            "Adam warmup time per epoch: 0.329659 s\n",
            "Best Adam train loss = 18.972, best Adam val loss = 18.972\n",
            "Training loss: 18.494265501813157\n",
            "Training loss: 18.039407606524005\n",
            "Training loss: 17.678599295741176\n",
            "Training loss: 17.288718930752342\n",
            "Training loss: 16.55709111518638\n",
            "Training loss: 16.164479759182992\n",
            "Training loss: 15.54744026755248\n",
            "Training loss: 15.208960384478987\n",
            "Training loss: 14.711383557128483\n",
            "Training loss: 14.313431109320746\n",
            "Training loss: 13.950014665850473\n",
            "Training loss: 13.779922436965963\n",
            "Training loss: 13.58820220483995\n",
            "Training loss: 13.496656997708438\n",
            "Training loss: 13.353754681336023\n",
            "Training loss: 13.204813117480313\n",
            "Training loss: 12.848238682481913\n",
            "Training loss: 12.74035463470203\n",
            "Training loss: 12.522469184781915\n",
            "Training loss: 12.096979919529446\n",
            "Training loss: 11.782593239191824\n",
            "Training loss: 11.518494294871951\n",
            "Training loss: 11.397046291660795\n",
            "Training loss: 11.274081238874968\n",
            "Training loss: 11.124214621222091\n",
            "Training loss: 11.089662320132016\n",
            "Training loss: 11.065907080508834\n",
            "Training loss: 10.999854053894728\n",
            "Training loss: 10.960243676924852\n",
            "Training loss: 10.924364432422903\n",
            "Training loss: 10.901919966944616\n",
            "Training loss: 10.820459074354646\n",
            "Training loss: 10.786662027157183\n",
            "Training loss: 10.74195781316693\n",
            "Training loss: 10.72364044291292\n",
            "Training loss: 10.71214579033367\n",
            "Training loss: 10.707123987622774\n",
            "Training loss: 10.702275062613237\n",
            "Training loss: 10.694031508628196\n",
            "Training loss: 10.681087111984487\n",
            "Training loss: 10.670474724682798\n",
            "Training loss: 10.640698605813531\n",
            "Training loss: 10.611905984674946\n",
            "Training loss: 10.605411688152726\n",
            "Training loss: 10.602820479459277\n",
            "Training loss: 10.587371257060004\n",
            "Training loss: 10.583148407828892\n",
            "Training loss: 10.581687036966182\n",
            "Training loss: 10.579423096685233\n",
            "Training loss: 10.573319421572567\n",
            "Training loss: 10.558534449343297\n",
            "Training loss: 10.53680469154811\n",
            "Training loss: 10.51749694300017\n",
            "Training loss: 10.479324853935086\n",
            "Training loss: 10.446249023450303\n",
            "Training loss: 10.420375212969414\n",
            "Training loss: 10.411356819587862\n",
            "Training loss: 10.404366065290928\n",
            "Training loss: 10.367942273707623\n",
            "Training loss: 10.344451260888091\n",
            "Training loss: 10.300824053777838\n",
            "Training loss: 10.286336246245872\n",
            "Training loss: 10.272414495024234\n",
            "Training loss: 10.24545411683695\n",
            "Training loss: 10.21733042717087\n",
            "Training loss: 10.18883835078214\n",
            "Training loss: 10.168997309909319\n",
            "Training loss: 10.167397387723646\n",
            "Training loss: 10.154980903484121\n",
            "Training loss: 10.151400729211284\n",
            "Training loss: 10.143414090495408\n",
            "Training loss: 10.12079577875706\n",
            "Training loss: 10.109864077590892\n",
            "Training loss: 10.086305243582633\n",
            "Training loss: 10.080670230301049\n",
            "Training loss: 10.06923559440925\n",
            "Training loss: 10.059081827908631\n",
            "Training loss: 10.053293155115677\n",
            "Training loss: 10.046698005429494\n",
            "Training loss: 10.043431921143851\n",
            "Training loss: 10.035405248828695\n",
            "Training loss: 10.03039832734698\n",
            "Training loss: 10.022605698649569\n",
            "Training loss: 10.01658815049957\n",
            "Training loss: 10.013803501265034\n",
            "Training loss: 10.01286982966764\n",
            "Training loss: 10.011008495468358\n",
            "Training loss: 10.009177585904112\n",
            "Training loss: 10.00418622384653\n",
            "Training loss: 9.995126304777221\n",
            "Training loss: 9.982271044393071\n",
            "Training loss: 9.978580648561351\n",
            "Training loss: 9.969139653559452\n",
            "Training loss: 9.96543219457993\n",
            "Training loss: 9.964238765306245\n",
            "Training loss: 9.962092581218029\n",
            "Training loss: 9.960730328380933\n",
            "Training loss: 9.959099625827912\n",
            "Training loss: 9.95762557762039\n",
            "Training loss: 9.956236129431355\n",
            "Training loss: 9.951026090794228\n",
            "Training loss: 9.943669587459082\n",
            "Training loss: 9.933234092533693\n",
            "Training loss: 9.929234439862892\n",
            "Training loss: 9.918854725298182\n",
            "Training loss: 9.91361153501844\n",
            "Training loss: 9.907473036512823\n",
            "Training loss: 9.899582838544612\n",
            "Training loss: 9.894523915001564\n",
            "Training loss: 9.882024823515163\n",
            "Training loss: 9.873970120036102\n",
            "Training loss: 9.867814513205658\n",
            "Training loss: 9.852049073009262\n",
            "Training loss: 9.837064410222274\n",
            "Training loss: 9.833396861061026\n",
            "Training loss: 9.82523160531941\n",
            "Training loss: 9.824049216432128\n",
            "Training loss: 9.823161559627223\n",
            "Training loss: 9.821026356004984\n",
            "Training loss: 9.815759713219352\n",
            "Training loss: 9.812801368891469\n",
            "Training loss: 9.804923777619452\n",
            "Training loss: 9.799285298586565\n",
            "Training loss: 9.78832250620954\n",
            "Training loss: 9.785522957818781\n",
            "Training loss: 9.779577299689391\n",
            "Training loss: 9.77433946712005\n",
            "Training loss: 9.765393659257526\n",
            "Training loss: 9.756985979097092\n",
            "Training loss: 9.741290676875899\n",
            "Training loss: 9.732919229748802\n",
            "Training loss: 9.729593775784124\n",
            "Training loss: 9.727727249265783\n",
            "Training loss: 9.720027524262404\n",
            "Training loss: 9.706387114937593\n",
            "Training loss: 9.695684382116474\n",
            "Training loss: 9.687784489223985\n",
            "Training loss: 9.68011393652857\n",
            "Training loss: 9.670789758410342\n",
            "Training loss: 9.655803946276013\n",
            "Training loss: 9.629007674067644\n",
            "Training loss: 9.62522260651874\n",
            "Training loss: 9.612572163759621\n",
            "Training loss: 9.61152060107066\n",
            "Training loss: 9.609480620356416\n",
            "Training loss: 9.608174016157584\n",
            "Training loss: 9.603233107748666\n",
            "Training loss: 9.592808521591811\n",
            "Training loss: 9.580017364860893\n",
            "Training loss: 9.56055695729022\n",
            "Training loss: 9.539983570687792\n",
            "Training loss: 9.500290286667079\n",
            "Training loss: 9.487307142721654\n",
            "Training loss: 9.470345649761487\n",
            "Training loss: 9.459373094139998\n",
            "Training loss: 9.454611524620828\n",
            "Training loss: 9.444596776632174\n",
            "Training loss: 9.43331799031237\n",
            "Training loss: 9.424888097313845\n",
            "Training loss: 9.414448893547403\n",
            "Training loss: 9.40698859852398\n",
            "Training loss: 9.404470403469\n",
            "Training loss: 9.403048380387274\n",
            "Training loss: 9.40156068723725\n",
            "Training loss: 9.399355829748005\n",
            "Training loss: 9.395956912915983\n",
            "Training loss: 9.39010805180904\n",
            "Training loss: 9.386500607315693\n",
            "Training loss: 9.383277158710802\n",
            "Training loss: 9.376343604881198\n",
            "Training loss: 9.366624890847346\n",
            "Training loss: 9.36031946322875\n",
            "Training loss: 9.358135403777476\n",
            "Training loss: 9.35770944389042\n",
            "Training loss: 9.356537876558377\n",
            "Training loss: 9.356289850551812\n",
            "Training loss: 9.355655931974653\n",
            "Training loss: 9.35509375538349\n",
            "Training loss: 9.354461757146598\n",
            "Training loss: 9.353379246035832\n",
            "Training loss: 9.352893064381286\n",
            "Training loss: 9.351637458540962\n",
            "Training loss: 9.350466634226311\n",
            "Training loss: 9.350135846140656\n",
            "Training loss: 9.349961522925705\n",
            "Training loss: 9.349877449399978\n",
            "Training loss: 9.349598269083707\n",
            "Training loss: 9.34945837688996\n",
            "Training loss: 9.349327830845764\n",
            "Training loss: 9.349234024054377\n",
            "Training loss: 9.349057102055193\n",
            "Training loss: 9.348986533357243\n",
            "Training loss: 9.348721409051882\n",
            "Training loss: 9.34840851871623\n",
            "Training loss: 9.347283613413861\n",
            "Training loss: 9.345798962373511\n",
            "Training loss: 9.34254884903952\n",
            "Training loss: 9.338427579727682\n",
            "Training loss: 9.336753301298206\n",
            "Training loss: 9.336188131787958\n",
            "Training loss: 9.336013918740273\n",
            "Training loss: 9.335925540129047\n",
            "Training loss: 9.335659713272106\n",
            "Training loss: 9.335419376386024\n",
            "Training loss: 9.334411899770927\n",
            "Training loss: 9.333715405786345\n",
            "Training loss: 9.33292462238505\n",
            "Training loss: 9.332231928615512\n",
            "Training loss: 9.33146806215359\n",
            "Training loss: 9.329504111280473\n",
            "Training loss: 9.329030140876913\n",
            "Training loss: 9.327732063470258\n",
            "Training loss: 9.326001137235183\n",
            "Training loss: 9.32433356588838\n",
            "Training loss: 9.322359083336632\n",
            "Training loss: 9.319527998153896\n",
            "Training loss: 9.31714292128465\n",
            "Training loss: 9.31366029473584\n",
            "Training loss: 9.312103852313747\n",
            "Training loss: 9.310963302818152\n",
            "Training loss: 9.30957466636528\n",
            "Training loss: 9.307653883310241\n",
            "Training loss: 9.30542933100294\n",
            "Training loss: 9.302576173512648\n",
            "Training loss: 9.298842734295729\n",
            "Training loss: 9.298687985875745\n",
            "Training loss: 9.296277977238267\n",
            "Training loss: 9.295612494717206\n",
            "Training loss: 9.29522921877218\n",
            "Training loss: 9.29476073760186\n",
            "Training loss: 9.294475367186656\n",
            "Training loss: 9.293984723825151\n",
            "Training loss: 9.2936623401019\n",
            "Training loss: 9.293418670816589\n",
            "Training loss: 9.292645519439983\n",
            "Training loss: 9.2917659008271\n",
            "Training loss: 9.290484747407326\n",
            "Training loss: 9.290356101915345\n",
            "Training loss: 9.290158350669712\n",
            "Training loss: 9.290098289201834\n",
            "Training loss: 9.28996535497814\n",
            "Training loss: 9.289669203739612\n",
            "Training loss: 9.289165287610201\n",
            "Training loss: 9.288486298478917\n",
            "Training loss: 9.287134721312293\n",
            "Training loss: 9.285805041898122\n",
            "Training loss: 9.284909467488239\n",
            "Training loss: 9.284646560777562\n",
            "Training loss: 9.284425360124613\n",
            "Training loss: 9.283882343207553\n",
            "Training loss: 9.282979568490228\n",
            "Training loss: 9.28187213551434\n",
            "Training loss: 9.280750563102467\n",
            "Training loss: 9.279375476673007\n",
            "Training loss: 9.27812225665211\n",
            "Training loss: 9.276543451986333\n",
            "Training loss: 9.275737573206086\n",
            "Training loss: 9.274596082464493\n",
            "Training loss: 9.27429994703599\n",
            "Training loss: 9.273239146202318\n",
            "Training loss: 9.269649509179455\n",
            "Training loss: 9.266444389978302\n",
            "Training loss: 9.266092565294215\n",
            "Training loss: 9.265369849175439\n",
            "Training loss: 9.26514653655449\n",
            "Training loss: 9.264933379167832\n",
            "Training loss: 9.264472245038382\n",
            "Training loss: 9.264250899399348\n",
            "Training loss: 9.264157891525437\n",
            "Training loss: 9.264099402090748\n",
            "Training loss: 9.263962719904097\n",
            "Training loss: 9.263661277883449\n",
            "Training loss: 9.263088858493601\n",
            "Training loss: 9.261955437450998\n",
            "Training loss: 9.261135939231306\n",
            "Training loss: 9.260608717239846\n",
            "Training loss: 9.260216802847356\n",
            "Training loss: 9.260054240978045\n",
            "Training loss: 9.259812840633602\n",
            "Training loss: 9.259788144199819\n",
            "Training loss: 9.259484198271254\n",
            "Training loss: 9.2594111012363\n",
            "Training loss: 9.259290480457883\n",
            "Training loss: 9.258890462972282\n",
            "Training loss: 9.25804947135734\n",
            "Training loss: 9.25712451311799\n",
            "Training loss: 9.255606912413967\n",
            "Training loss: 9.254806758883195\n",
            "Training loss: 9.254211063118394\n",
            "Training loss: 9.253961382960263\n",
            "Training loss: 9.253728020824783\n",
            "Training loss: 9.253678454679262\n",
            "Training loss: 9.253535916360207\n",
            "Training loss: 9.252658878555266\n",
            "Training loss: 9.250752562160944\n",
            "Training loss: 9.249483764058708\n",
            "Training loss: 9.248773137642218\n",
            "Training loss: 9.248410359502302\n",
            "Training loss: 9.248197364603497\n",
            "Training loss: 9.247916566045072\n",
            "Training loss: 9.247455051572523\n",
            "Training loss: 9.247047797034023\n",
            "Training loss: 9.246890322483305\n",
            "Training loss: 9.246198427221772\n",
            "Training loss: 9.245795085064415\n",
            "Training loss: 9.244739121828134\n",
            "Training loss: 9.243990624024276\n",
            "Training loss: 9.243644319558761\n",
            "Training loss: 9.242289939048666\n",
            "Training loss: 9.24111498562616\n",
            "Training loss: 9.240489536814096\n",
            "Training loss: 9.240260125279356\n",
            "Training loss: 9.2398543390426\n",
            "Training loss: 9.238712693430692\n",
            "Training loss: 9.237762800780006\n",
            "Training loss: 9.236688541991118\n",
            "Training loss: 9.235938032319307\n",
            "Training loss: 9.235809395220047\n",
            "Training loss: 9.234316434644523\n",
            "Training loss: 9.234004690114542\n",
            "Training loss: 9.233692684666103\n",
            "Training loss: 9.233419881297925\n",
            "Training loss: 9.233120628935248\n",
            "Training loss: 9.232936271363213\n",
            "Training loss: 9.232298811834305\n",
            "Training loss: 9.231940460219036\n",
            "Training loss: 9.231654886955553\n",
            "Training loss: 9.23152908951655\n",
            "Training loss: 9.231370427633882\n",
            "Training loss: 9.231197220661155\n",
            "Training loss: 9.230925316782812\n",
            "Training loss: 9.230700763121735\n",
            "Training loss: 9.23067520378064\n",
            "Training loss: 9.230515312984622\n",
            "Training loss: 9.230446355189658\n",
            "Training loss: 9.230362310425155\n",
            "Training loss: 9.230264579067468\n",
            "Training loss: 9.230128199001454\n",
            "Training loss: 9.229998452541407\n",
            "Training loss: 9.229761632719303\n",
            "Training loss: 9.229602575871517\n",
            "Training loss: 9.229401119895014\n",
            "Training loss: 9.229095394981298\n",
            "Training loss: 9.228600322709225\n",
            "Training loss: 9.22796062549924\n",
            "Training loss: 9.227658530101715\n",
            "Training loss: 9.227038846178976\n",
            "Training loss: 9.226836708713002\n",
            "Training loss: 9.225773786765613\n",
            "Training loss: 9.225399486904188\n",
            "Training loss: 9.225038895936596\n",
            "Training loss: 9.224812330796356\n",
            "Training loss: 9.224463145442389\n",
            "Training loss: 9.224177787416178\n",
            "Training loss: 9.22393076867938\n",
            "Training loss: 9.223652617585072\n",
            "Training loss: 9.223577914018211\n",
            "Training loss: 9.223456912626553\n",
            "Training loss: 9.223329728718308\n",
            "Training loss: 9.223131325549561\n",
            "Training loss: 9.222967957692147\n",
            "Training loss: 9.222665433907808\n",
            "Training loss: 9.222381610081\n",
            "Training loss: 9.222062355193447\n",
            "Training loss: 9.221608468719545\n",
            "Training loss: 9.221500015190953\n",
            "Training loss: 9.221294704039003\n",
            "Training loss: 9.221288465933483\n",
            "Training loss: 9.221156492006479\n",
            "Training loss: 9.220961649094491\n",
            "Training loss: 9.220923180173395\n",
            "Training loss: 9.220697711146682\n",
            "Training loss: 9.22061202233307\n",
            "Training loss: 9.22049603359921\n",
            "Training loss: 9.220368272655888\n",
            "Training loss: 9.220306699566903\n",
            "Training loss: 9.22016217317968\n",
            "Training loss: 9.219984720236111\n",
            "Training loss: 9.219790292095446\n",
            "Training loss: 9.219497388030643\n",
            "Training loss: 9.21940587328821\n",
            "Training loss: 9.21935984393125\n",
            "Training loss: 9.21929744241739\n",
            "Training loss: 9.219290704952554\n",
            "Training loss: 9.21926528609412\n",
            "Training loss: 9.219255496457766\n",
            "Training loss: 9.219234680687864\n",
            "Training loss: 9.219220134962626\n",
            "Training loss: 9.219213972068681\n",
            "Training loss: 9.219198104446498\n",
            "Training loss: 9.219193734006435\n",
            "Training loss: 9.219183792343268\n",
            "Training loss: 9.219182082746538\n",
            "Training loss: 9.219172568964707\n",
            "Training loss: 9.219145887463231\n",
            "Training loss: 9.219130380057406\n",
            "Training loss: 9.219123983880452\n",
            "Training loss: 9.219122629346918\n",
            "Training loss: 9.219119897404203\n",
            "Training loss: 9.219110927716864\n",
            "Training loss: 9.219099434366173\n",
            "Training loss: 9.21909514459193\n",
            "Training loss: 9.219089825734669\n",
            "Training loss: 9.219081479198962\n",
            "Training loss: 9.219071859733091\n",
            "Training loss: 9.219027996829702\n",
            "Training loss: 9.21902190327218\n",
            "Training loss: 9.219015276539073\n",
            "Training loss: 9.219009967719293\n",
            "Training loss: 9.218994361558902\n",
            "Training loss: 9.218979827883793\n",
            "Training loss: 9.2189676621304\n",
            "Training loss: 9.218954836910632\n",
            "Training loss: 9.21892734285987\n",
            "Training loss: 9.218921049979125\n",
            "Training loss: 9.218913656268985\n",
            "Training loss: 9.21891318142908\n",
            "Training loss: 9.218909415577066\n",
            "Training loss: 9.218907107627194\n",
            "Training loss: 9.218904459006207\n",
            "Training loss: 9.218902957299447\n",
            "Training loss: 9.218902217445557\n",
            "Training loss: 9.218893957477828\n",
            "Training loss: 9.218882720096984\n",
            "Training loss: 9.218853125251139\n",
            "Training loss: 9.218816943163633\n",
            "Training loss: 9.218767149040021\n",
            "Training loss: 9.218745460570814\n",
            "Training loss: 9.218686624925217\n",
            "Training loss: 9.218653258701774\n",
            "Training loss: 9.218618582987576\n",
            "Training loss: 9.21857547760325\n",
            "Training loss: 9.218465086977888\n",
            "Training loss: 9.218389913664817\n",
            "Training loss: 9.218327231505022\n",
            "Training loss: 9.21825352480625\n",
            "Training loss: 9.218228641531885\n",
            "Training loss: 9.218222964711034\n",
            "Training loss: 9.218222842162366\n",
            "Training loss: 9.218221146735315\n",
            "Training loss: 9.218219450654047\n",
            "Training loss: 9.218217729847266\n",
            "Training loss: 9.21821583258088\n",
            "Training loss: 9.21821394194898\n",
            "Training loss: 9.218212541623869\n",
            "Training loss: 9.218210411939106\n",
            "Training loss: 9.218209893055795\n",
            "Training loss: 9.218208746608722\n",
            "Training loss: 9.218205828861366\n",
            "Training loss: 9.218202131912385\n",
            "Training loss: 9.218191001578663\n",
            "Training loss: 9.21819019463225\n",
            "Training loss: 9.218179478280483\n",
            "Training loss: 9.218168468979705\n",
            "Training loss: 9.218165212770812\n",
            "Training loss: 9.218158535778851\n",
            "Training loss: 9.218154651326335\n",
            "Training loss: 9.218151692595905\n",
            "Training loss: 9.218149625126552\n",
            "Best basinhopping iteration error so far: 9999\n",
            "Current basinhopping iteration error: 9.218149625126552\n",
            "==========\n",
            "\n",
            "Total training time: 127.70745134353638 seconds\n",
            "Time per epoch: 0.278229741489186 seconds\n",
            "\n",
            "OPTIMIZATION ERROR FOR N=3, L=1, â modes=[2 3]\n",
            "9.218149625126552\n",
            "PARAMETERS:\n",
            "[ 1.62701388e+00  2.80430781e-01  6.74279940e-01  7.61894295e-02\n",
            " -2.77750021e-01  7.25372366e-01  9.06841960e-01  9.32249223e-01\n",
            "  6.72703565e-01 -1.67017706e-01  1.02474274e+00  3.02995369e-01\n",
            "  2.42237544e-01  1.12760234e+00  1.53801705e-01  3.96907157e-01\n",
            "  1.36832092e+00  7.78440505e-02  6.12937216e-07  1.11554438e+00\n",
            "  2.63420197e-01 -3.63066484e-03  7.47173159e-02  4.48223042e-02\n",
            "  1.66318324e+00  2.36550291e+00  7.13917085e-02]\n",
            "=== ACTIVE LAYER 1 ===\n",
            "Q1 = [[-0.01089351+0.j -0.75320464+0.j  0.08151039+0.j -0.00724179+0.j\n",
            "   0.52495181+0.j -0.38767683+0.j]\n",
            " [-0.50235854+0.j -0.22071715+0.j -0.70339344+0.j  0.39463134+0.j\n",
            "  -0.21982683+0.j -0.00998919+0.j]\n",
            " [-0.76869646+0.j  0.04617457+0.j  0.4704536 +0.j  0.02892476+0.j\n",
            "   0.2406998 +0.j  0.35619382+0.j]\n",
            " [ 0.00724179+0.j -0.52495181+0.j  0.38767683+0.j -0.01089351+0.j\n",
            "  -0.75320464+0.j  0.08151039+0.j]\n",
            " [-0.39463134+0.j  0.21982683+0.j  0.00998919+0.j -0.50235854+0.j\n",
            "  -0.22071715+0.j -0.70339344+0.j]\n",
            " [-0.02892476+0.j -0.2406998 +0.j -0.35619382+0.j -0.76869646+0.j\n",
            "   0.04617457+0.j  0.4704536 +0.j]]\n",
            "Z = [[1.00000061+0.j 0.        +0.j 0.        +0.j 0.        +0.j\n",
            "  0.        +0.j 0.        +0.j]\n",
            " [0.        +0.j 3.05122874+0.j 0.        +0.j 0.        +0.j\n",
            "  0.        +0.j 0.        +0.j]\n",
            " [0.        +0.j 0.        +0.j 1.30137344+0.j 0.        +0.j\n",
            "  0.        +0.j 0.        +0.j]\n",
            " [0.        +0.j 0.        +0.j 0.        +0.j 0.99999939+0.j\n",
            "  0.        +0.j 0.        +0.j]\n",
            " [0.        +0.j 0.        +0.j 0.        +0.j 0.        +0.j\n",
            "  0.32773682+0.j 0.        +0.j]\n",
            " [0.        +0.j 0.        +0.j 0.        +0.j 0.        +0.j\n",
            "  0.        +0.j 0.76841894+0.j]]\n",
            "Q2 = [[-0.00226055+0.j -0.09148206+0.j -0.79668265+0.j  0.10219493+0.j\n",
            "  -0.35305847+0.j  0.47098683+0.j]\n",
            " [-0.02718926+0.j  0.15433459+0.j -0.32694422+0.j  0.02377277+0.j\n",
            "   0.91786658+0.j  0.15970236+0.j]\n",
            " [ 0.55887592+0.j -0.00505125+0.j  0.03428692+0.j  0.82213405+0.j\n",
            "   0.0256321 +0.j -0.09947463+0.j]\n",
            " [-0.10219493+0.j  0.35305847+0.j -0.47098683+0.j -0.00226055+0.j\n",
            "  -0.09148206+0.j -0.79668265+0.j]\n",
            " [-0.02377277+0.j -0.91786658+0.j -0.15970236+0.j -0.02718926+0.j\n",
            "   0.15433459+0.j -0.32694422+0.j]\n",
            " [-0.82213405+0.j -0.0256321 +0.j  0.09947463+0.j  0.55887592+0.j\n",
            "  -0.00505125+0.j  0.03428692+0.j]]\n",
            "Symplectic matrix:\n",
            "[[ 0.97315434+0.j -0.15075743+0.j -0.38205104+0.j -0.36131496+0.j\n",
            "  -0.22410209+0.j -0.10564645+0.j]\n",
            " [-0.03129716+0.j -0.07899486+0.j -0.5651087 +0.j -0.07198495+0.j\n",
            "  -0.29883898+0.j -0.29763834+0.j]\n",
            " [-0.02779527+0.j -0.82682123+0.j  0.42341982+0.j  0.03674284+0.j\n",
            "  -0.31710722+0.j -0.17547313+0.j]\n",
            " [-0.03937883+0.j -0.04714937+0.j -0.83754306+0.j  0.89380423+0.j\n",
            "  -0.45793785+0.j -0.45656434+0.j]\n",
            " [ 1.55404381+0.j  0.71231744+0.j  1.94967637+0.j -0.94304608+0.j\n",
            "   0.55086133+0.j -0.1928233 +0.j]\n",
            " [-0.04732698+0.j  0.34238687+0.j  0.25616343+0.j -0.04667484+0.j\n",
            "  -0.80259476+0.j  0.42472788+0.j]]\n",
            "Symplecticity condition: [[ 0.00000000e+00+0.j  2.22044605e-16+0.j -2.22044605e-16+0.j]\n",
            " [-2.22044605e-16+0.j  0.00000000e+00+0.j -1.24900090e-16+0.j]\n",
            " [ 2.22044605e-16+0.j  1.24900090e-16+0.j  0.00000000e+00+0.j]]\n",
            "True\n",
            "Orthogonality condition: [[ 1.3076516 +0.j  0.32177657+0.j  0.01215914+0.j]\n",
            " [ 0.32177657+0.j  0.5096427 +0.j -0.02874709+0.j]\n",
            " [ 0.01215914+0.j -0.02874709+0.j  0.9963881 +0.j]]\n",
            "False\n",
            "\n",
            "Displacement vector:\n",
            "[-0.00363066+0.j  0.07471732+0.j  0.0448223 +0.j  1.66318324+0.j\n",
            "  2.36550291+0.j  0.07139171+0.j]\n",
            "Trained model: fock_cubicphase_gamma0.2_trainsize50_rng-2.0to2.0_N3_L1_sub[[1, 2]]_inNone\n",
            "Final training loss: 9.218149625126552\n",
            "Number of tunable parameters: 44\n",
            "Number of terms for each trace: [1 1 1 1 1 1 1]\n",
            "\n",
            "=== Phase 1: Adam/AdamW warmup (JAX) ===\n",
            "[Adam] Epoch 0/200 | train loss = 701.449 | val loss = 701.449\n",
            "[Adam] Epoch 1/200 | train loss = 701.449 | val loss = 701.449\n",
            "[Adam] Epoch 2/200 | train loss = 648.712 | val loss = 648.712\n",
            "[Adam] Epoch 3/200 | train loss = 605.83 | val loss = 605.83\n",
            "[Adam] Epoch 4/200 | train loss = 569.1 | val loss = 569.1\n",
            "[Adam] Epoch 5/200 | train loss = 532.234 | val loss = 532.234\n",
            "[Adam] Epoch 6/200 | train loss = 489.564 | val loss = 489.564\n",
            "[Adam] Epoch 7/200 | train loss = 443.939 | val loss = 443.939\n",
            "[Adam] Epoch 8/200 | train loss = 401.717 | val loss = 401.717\n",
            "[Adam] Epoch 9/200 | train loss = 366.494 | val loss = 366.494\n",
            "[Adam] Epoch 10/200 | train loss = 340.319 | val loss = 340.319\n",
            "[Adam] Epoch 11/200 | train loss = 325.779 | val loss = 325.779\n",
            "[Adam] Epoch 12/200 | train loss = 323.945 | val loss = 323.945\n",
            "[Adam] Epoch 13/200 | train loss = 329.539 | val loss = 329.539\n",
            "[Adam] Epoch 14/200 | train loss = 332.456 | val loss = 332.456\n",
            "[Adam] Epoch 15/200 | train loss = 328.087 | val loss = 328.087\n",
            "[Adam] Epoch 16/200 | train loss = 319.184 | val loss = 319.184\n",
            "[Adam] Epoch 17/200 | train loss = 310.017 | val loss = 310.017\n",
            "[Adam] Epoch 18/200 | train loss = 302.392 | val loss = 302.392\n",
            "[Adam] Epoch 19/200 | train loss = 296.274 | val loss = 296.274\n",
            "[Adam] Epoch 20/200 | train loss = 292.188 | val loss = 292.188\n",
            "[Adam] Epoch 21/200 | train loss = 291.316 | val loss = 291.316\n",
            "[Adam] Epoch 22/200 | train loss = 293.029 | val loss = 293.029\n",
            "[Adam] Epoch 23/200 | train loss = 293.69 | val loss = 293.69\n",
            "[Adam] Epoch 24/200 | train loss = 290.562 | val loss = 290.562\n",
            "[Adam] Epoch 25/200 | train loss = 284.698 | val loss = 284.698\n",
            "[Adam] Epoch 26/200 | train loss = 278.619 | val loss = 278.619\n",
            "[Adam] Epoch 27/200 | train loss = 273.404 | val loss = 273.404\n",
            "[Adam] Epoch 28/200 | train loss = 268.459 | val loss = 268.459\n",
            "[Adam] Epoch 29/200 | train loss = 263.092 | val loss = 263.092\n",
            "[Adam] Epoch 30/200 | train loss = 257.565 | val loss = 257.565\n",
            "[Adam] Epoch 31/200 | train loss = 252.747 | val loss = 252.747\n",
            "[Adam] Epoch 32/200 | train loss = 248.986 | val loss = 248.986\n",
            "[Adam] Epoch 33/200 | train loss = 245.421 | val loss = 245.421\n",
            "[Adam] Epoch 34/200 | train loss = 240.884 | val loss = 240.884\n",
            "[Adam] Epoch 35/200 | train loss = 235.388 | val loss = 235.388\n",
            "[Adam] Epoch 36/200 | train loss = 229.922 | val loss = 229.922\n",
            "[Adam] Epoch 37/200 | train loss = 225.05 | val loss = 225.05\n",
            "[Adam] Epoch 38/200 | train loss = 220.411 | val loss = 220.411\n",
            "[Adam] Epoch 39/200 | train loss = 215.551 | val loss = 215.551\n",
            "[Adam] Epoch 40/200 | train loss = 210.589 | val loss = 210.589\n",
            "[Adam] Epoch 41/200 | train loss = 205.668 | val loss = 205.668\n",
            "[Adam] Epoch 42/200 | train loss = 200.409 | val loss = 200.409\n",
            "[Adam] Epoch 43/200 | train loss = 194.79 | val loss = 194.79\n",
            "[Adam] Epoch 44/200 | train loss = 189.525 | val loss = 189.525\n",
            "[Adam] Epoch 45/200 | train loss = 184.926 | val loss = 184.926\n",
            "[Adam] Epoch 46/200 | train loss = 180.653 | val loss = 180.653\n",
            "[Adam] Epoch 47/200 | train loss = 176.504 | val loss = 176.504\n",
            "[Adam] Epoch 48/200 | train loss = 172.582 | val loss = 172.582\n",
            "[Adam] Epoch 49/200 | train loss = 168.822 | val loss = 168.822\n",
            "[Adam] Epoch 50/200 | train loss = 165.047 | val loss = 165.047\n",
            "[Adam] Epoch 51/200 | train loss = 161.474 | val loss = 161.474\n",
            "[Adam] Epoch 52/200 | train loss = 158.42 | val loss = 158.42\n",
            "[Adam] Epoch 53/200 | train loss = 155.77 | val loss = 155.77\n",
            "[Adam] Epoch 54/200 | train loss = 153.277 | val loss = 153.277\n",
            "[Adam] Epoch 55/200 | train loss = 150.957 | val loss = 150.957\n",
            "[Adam] Epoch 56/200 | train loss = 148.879 | val loss = 148.879\n",
            "[Adam] Epoch 57/200 | train loss = 146.987 | val loss = 146.987\n",
            "[Adam] Epoch 58/200 | train loss = 145.302 | val loss = 145.302\n",
            "[Adam] Epoch 59/200 | train loss = 143.902 | val loss = 143.902\n",
            "[Adam] Epoch 60/200 | train loss = 142.7 | val loss = 142.7\n",
            "[Adam] Epoch 61/200 | train loss = 141.543 | val loss = 141.543\n",
            "[Adam] Epoch 62/200 | train loss = 140.408 | val loss = 140.408\n",
            "[Adam] Epoch 63/200 | train loss = 139.383 | val loss = 139.383\n",
            "[Adam] Epoch 64/200 | train loss = 138.504 | val loss = 138.504\n",
            "[Adam] Epoch 65/200 | train loss = 137.718 | val loss = 137.718\n",
            "[Adam] Epoch 66/200 | train loss = 136.956 | val loss = 136.956\n",
            "[Adam] Epoch 67/200 | train loss = 136.167 | val loss = 136.167\n",
            "[Adam] Epoch 68/200 | train loss = 135.317 | val loss = 135.317\n",
            "[Adam] Epoch 69/200 | train loss = 134.414 | val loss = 134.414\n",
            "[Adam] Epoch 70/200 | train loss = 133.506 | val loss = 133.506\n",
            "[Adam] Epoch 71/200 | train loss = 132.604 | val loss = 132.604\n",
            "[Adam] Epoch 72/200 | train loss = 131.666 | val loss = 131.666\n",
            "[Adam] Epoch 73/200 | train loss = 130.664 | val loss = 130.664\n",
            "[Adam] Epoch 74/200 | train loss = 129.636 | val loss = 129.636\n",
            "[Adam] Epoch 75/200 | train loss = 128.627 | val loss = 128.627\n",
            "[Adam] Epoch 76/200 | train loss = 127.648 | val loss = 127.648\n",
            "[Adam] Epoch 77/200 | train loss = 126.692 | val loss = 126.692\n",
            "[Adam] Epoch 78/200 | train loss = 125.745 | val loss = 125.745\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m postprocessors \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# === BUILD, TRAIN AND TEST QNN ===\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m qnn, train_loss, valid_loss \u001b[38;5;241m=\u001b[39m \u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43ml\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mladder_modes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_addition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobservable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_initial_squeezing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_initial_mixing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_passive_gaussian\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbasinhopping_iters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_preprocessors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_preprocessors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpostprocessors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_pars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m qnn_test_outputs, norm, loss_value \u001b[38;5;241m=\u001b[39m qnn\u001b[38;5;241m.\u001b[39mtest_model(train_dataset[\u001b[38;5;241m0\u001b[39m], train_dataset[\u001b[38;5;241m1\u001b[39m], loss_function)\n\u001b[1;32m     64\u001b[0m qnns\u001b[38;5;241m.\u001b[39mappend(qnn)\n",
            "File \u001b[0;32m~/QuanticGit/Quannto/quannto/core/qnn_trainers.py:422\u001b[0m, in \u001b[0;36mhybrid_build_and_train_model\u001b[0;34m(model_name, N, layers, n_inputs, n_outputs, ladder_modes, is_addition, observable, include_initial_squeezing, include_initial_mixing, is_passive_gaussian, train_set, valid_set, loss_function, hopping_iters, in_preprocs, out_prepocs, postprocs, init_pars, save, adam_epochs, adam_learning_rate, adam_weight_decay)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, adam_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    421\u001b[0m     params, opt_state, loss_val \u001b[38;5;241m=\u001b[39m adam_step(params, opt_state)\n\u001b[0;32m--> 422\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloss_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m validate_QNN \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    425\u001b[0m         val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(validate_QNN(np\u001b[38;5;241m.\u001b[39marray(params)))\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/array.py:310\u001b[0m, in \u001b[0;36mArrayImpl.__float__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__float__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    309\u001b[0m   core\u001b[38;5;241m.\u001b[39mcheck_scalar_conversion(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 310\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_value\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__float__\u001b[39m()\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/profiler.py:354\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    353\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/jax/_src/array.py:644\u001b[0m, in \u001b[0;36mArrayImpl._value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    642\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fully_replicated \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    643\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msharding\u001b[38;5;241m.\u001b[39m_internal_device_list\u001b[38;5;241m.\u001b[39maddressable_device_list):  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m     npy_value, did_copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_single_device_array_to_np_array_did_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    645\u001b[0m     npy_value\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m did_copy:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "qnns = []\n",
        "train_losses = []\n",
        "qnns_outs = []\n",
        "legend_labels = []\n",
        "\n",
        "for (N, l, ladder_modes, is_addition, in_norm_range) in zip(\n",
        "    qnns_modes,\n",
        "    qnns_layers,\n",
        "    qnns_ladder_modes,\n",
        "    qnns_is_addition,\n",
        "    in_norm_ranges,\n",
        "):\n",
        "    # === NAME AND LEGEND OF THE QONN MODEL ===\n",
        "    model_name = (\n",
        "        task_name\n",
        "        + \"_N\" + str(N)\n",
        "        + \"_L\" + str(l)\n",
        "        + (\"_add\" if is_addition else \"_sub\")\n",
        "        + str(ladder_modes)\n",
        "        + \"_in\" + str(in_norm_range)\n",
        "    )\n",
        "\n",
        "    nongauss_op = r\"$\\hat a^\\dagger$\" if is_addition else r\"$\\hat a$\"\n",
        "    legend_labels.append(\n",
        "        f\"N={N}, L={l}, {nongauss_op} in modes {np.array(ladder_modes[0]) + 1}\"\n",
        "    )\n",
        "\n",
        "    # === PREPROCESSORS AND POSTPROCESSORS ===\n",
        "    in_preprocessors = []\n",
        "    if in_norm_range is not None:\n",
        "        in_preprocessors.append(\n",
        "            partial(rescale_data, data_range=input_range, scale_data_range=in_norm_range)\n",
        "        )\n",
        "    in_preprocessors.append(partial(pad_data, length=2 * N))\n",
        "\n",
        "    out_preprocessors = []\n",
        "    postprocessors = []\n",
        "\n",
        "    # === BUILD, TRAIN AND TEST QNN ===\n",
        "    qnn, train_loss, valid_loss = optimize(\n",
        "        model_name,\n",
        "        N,\n",
        "        l,\n",
        "        n_inputs,\n",
        "        n_outputs,\n",
        "        ladder_modes,\n",
        "        is_addition,\n",
        "        observable,\n",
        "        include_initial_squeezing,\n",
        "        include_initial_mixing,\n",
        "        is_passive_gaussian,\n",
        "        train_dataset,\n",
        "        None,\n",
        "        loss_function,\n",
        "        basinhopping_iters,\n",
        "        in_preprocessors,\n",
        "        out_preprocessors,\n",
        "        postprocessors,\n",
        "        init_pars=params,\n",
        "    )\n",
        "\n",
        "    qnn_test_outputs, norm, loss_value = qnn.test_model(train_dataset[0], train_dataset[1], loss_function)\n",
        "\n",
        "    qnns.append(qnn)\n",
        "    train_losses.append(train_loss.copy())\n",
        "    qnns_outs.append(qnn_test_outputs.copy())\n",
        "\n",
        "    print(f\"Trained model: {model_name}\")\n",
        "    print(f\"Final training loss: {train_loss[-1] if len(train_loss) else train_loss}\")\n",
        "\n",
        "    # === SAVE QNN MODEL RESULTS ===\n",
        "    np.save(models_train_losses_path(model_name, \"npy\"), np.array(train_loss))\n",
        "    np.save(models_testing_results_path(model_name, \"npy\"), np.array(qnn_test_outputs))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb6c01aa",
      "metadata": {},
      "source": [
        "## 6) Plot training losses\n",
        "\n",
        "We compare the training trajectories for all mode counts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a992624e",
      "metadata": {},
      "outputs": [],
      "source": [
        "nongauss_ops = [r\"$\\hat a^\\dagger$\" if is_addition else r\"$\\hat a$\" for is_addition in qnns_is_addition]\n",
        "\n",
        "filename = (\n",
        "    task_name\n",
        "    + \"_N\" + str(qnns_modes)\n",
        "    + \"_L\" + str(qnns_layers)\n",
        "    + \"_ph\" + str(nongauss_ops)\n",
        "    + str(qnns_ladder_modes)\n",
        ")\n",
        "\n",
        "plot_qnns_loglosses(train_losses, None, legend_labels, filename)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49895144",
      "metadata": {},
      "source": [
        "## Next steps\n",
        "\n",
        "- Add a validation dataset and track generalization.\n",
        "- Increase expressivity by adding layers or changing ladder-mode patterns.\n",
        "- Try different `gamma` values and compare learnability.\n",
        "- Save and plot model outputs vs. targets to visually assess fit quality.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
